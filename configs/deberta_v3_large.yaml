seed : -1

project_name : "PII"

kfold_name : fold_msk_5_seed_42
selected_folds : [0,1,2,3,4] 

name : ""
exp_name : "" 
checkpoints_path : ""  
device : 0

dataset : FeedbackDataset
data:
  params_train :
    add_text_prob : 0.5
    replace_text_prob : 0.5
    use_re : false
  params_valid :
    add_text_prob : 0.0
    replace_text_prob : 0.0
    use_re : false
model:
  model_params : 
    model_name : microsoft/deberta-large
    num_labels : 8
    use_dropout : false
    use_gradient_checkpointing : true
    config_path : 
    pretrained_path : 
    max_len : 4096
    pooling_params :
      pooling_name: MeanPooling
      params : {}
    
  pretrained_weights : 
  pretrained_tokenizer : 

  loss:
    loss_name : nn.CrossEntropyLoss
    loss_params : {"reduction":"mean"}

  lossy:
    loss_name : nn.BCEWithLogitsLoss
    loss_params : {"reduction":"mean"}

optimizer:
  name : optim.AdamW
  params : 
    lr: 0.000004
    betas: [0.9, 0.999]
    eps: 0.000001
    weight_decay: 0.04

scheduler:
  name: poly
  params : 
    lr_end: 0.0000007
    power: 3
  warmup: 0.04

train_loader:
  batch_size: 1
  drop_last: true
  num_workers: 16
  pin_memory: false
  shuffle: true

val_loader:
  batch_size: 1
  drop_last: false
  num_workers: 16
  pin_memory: false
  shuffle: false

trainer:
  use_amp: false
  epochs: 6
  sample: false
  train_all_data : false
  use_awp: false
  ema_decay_rate : 0
  grad_clip: false
  max_norm: 1

callbacks:
  save : true
  es: true
  patience: 15
  verbose_eval: 1
  epoch_pct_eval: 0.5
  epoch_eval_dist: "uniforme"
  metric_track: f5_micro
  mode: max
  top_k: 1
  start_eval_epoch : 0
  save_last_k : 0
  use_wnb : false
