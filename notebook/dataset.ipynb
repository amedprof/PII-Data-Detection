{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84bf958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895aef6",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0104cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.json', 'test.json', 'sample_submission.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(r\"/database/kaggle/PII/data\")\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c19b120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(data_path/'train.json')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bfe6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text                                             tokens                                trailing_whitespace                                             labels\n",
       "0         7  Design Thinking for innovation reflexion-Avril...  [Design, Thinking, for, innovation, reflexion,...  [True, True, True, True, False, False, True, F...  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...\n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...  [True, False, False, True, True, False, False,...  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0ec52",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c48d18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pylab import cm, matplotlib\n",
    "import os\n",
    "\n",
    "colors = {\n",
    "            'NAME_STUDENT': '#8000ff',\n",
    "            'EMAIL': '#2b7ff6',\n",
    "            'USERNAME': '#2adddd',\n",
    "            'ID_NUM': '#80ffb4',\n",
    "            'PHONE_NUM': 'd4dd80',\n",
    "            'URL_PERSONAL': '#ff8042',\n",
    "            'STREET_ADDRESS': '#ff0000'\n",
    "         }\n",
    "\n",
    "\n",
    "def visualize(full_text,offset_mapping,labels):\n",
    "    \n",
    "    ents = []\n",
    "    for offset,lab in zip(offset_mapping,labels):\n",
    "        ents.append({\n",
    "                        'start': int(offset[0]), \n",
    "                         'end': int(offset[1]), \n",
    "                         'label': str(lab.split('-')[1]) #+ ' - ' + str(row['discourse_effectiveness'])\n",
    "                    })\n",
    "\n",
    "    doc2 = {\n",
    "        \"text\": full_text,\n",
    "        \"ents\": ents,\n",
    "#         \"title\": \"idx\"\n",
    "    }\n",
    "\n",
    "    options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22777e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75a8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from text_unidecode import unidecode\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(u'\\x9d', u' ')\n",
    "    text = resolve_encodings_and_normalize(text)\n",
    "    # text = text.replace(u'\\xa0', u' ')\n",
    "    # text = text.replace(u'\\x85', u'\\n')\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def add_text_to_df(test_df,data_folder):\n",
    "    mapper = {}\n",
    "    for idx in tqdm(test_df.essay_id.unique()):\n",
    "        with open(data_folder/f'{idx}.txt','r') as f:\n",
    "            texte = clean_text(f.read())\n",
    "            # texte = resolve_encodings_and_normalize(f.read())\n",
    "            # texte = texte.strip() \n",
    "        mapper[idx] = texte\n",
    "\n",
    "    test_df['discourse_ids'] = np.arange(len(test_df))\n",
    "    test_df['essay_text'] = test_df['essay_id'].map(mapper)\n",
    "    test_df['discourse_text'] = test_df['discourse_text'].transform(clean_text)\n",
    "    test_df['discourse_text'] = test_df['discourse_text'].str.strip()\n",
    "\n",
    "    test_df['previous_discourse_end'] = 0\n",
    "    test_df['st_ed'] = test_df.apply(get_start_end('discourse_text'),axis=1)\n",
    "    test_df['discourse_start'] = test_df['st_ed'].transform(lambda x:x[0])\n",
    "    test_df['discourse_end'] = test_df['st_ed'].transform(lambda x:x[1])\n",
    "    test_df['previous_discourse_end'] = test_df.groupby(\"essay_id\")['discourse_end'].transform(lambda x:x.shift(1).fillna(0)).astype(int)\n",
    "    test_df['st_ed'] = test_df.apply(get_start_end('discourse_text'),axis=1)\n",
    "    test_df['discourse_start'] = test_df['st_ed'].transform(lambda x:x[0]) #+ test_df['previous_discourse_end']\n",
    "    test_df['discourse_end'] = test_df['st_ed'].transform(lambda x:x[1]) #+ test_df['previous_discourse_end']\n",
    "\n",
    "    if 'target' in test_df.columns:\n",
    "        classe_mapper = {'Ineffective':0,\"Adequate\":1,\"Effective\":2}\n",
    "        test_df['target'] = test_df['discourse_effectiveness'].map(classe_mapper)\n",
    "        \n",
    "    else:\n",
    "        test_df['target'] = 1 \n",
    "\n",
    "    return test_df\n",
    "\n",
    "def get_essays(df,n_cpu=4):\n",
    "    \n",
    "    pool = joblib.Parallel(n_cpu)\n",
    "    mapper = joblib.delayed(_get_essay)\n",
    "    tasks = [mapper(df) for idx,df in df.groupby('id')]\n",
    "    ids = [idx for idx,_ in df.groupby('id')]\n",
    "    \n",
    "    return pd.DataFrame({\"id\":ids,\"essay\":pool(tqdm(tasks))})\n",
    "\n",
    "def _get_essay(df):\n",
    "    text_recons = ''\n",
    "    for i, (id_,row) in enumerate(df.iterrows()):\n",
    "    #     print(row)\n",
    "        activity = row[\"activity\"]\n",
    "        curs_pos = row[\"cursor_position\"] # cursor position AFTER activity!\n",
    "        text_change = row[\"text_change\"]\n",
    "\n",
    "\n",
    "        if activity == 'Input' or activity == 'Paste':\n",
    "            text_recons = text_recons[:curs_pos - len(text_change)] + text_change + text_recons[curs_pos - len(text_change):]   \n",
    "        if activity == 'Remove/Cut':\n",
    "            text_recons = text_recons[:curs_pos] + text_recons[curs_pos + len(text_change):]\n",
    "        if activity == 'Replace': # Combined remove and input operation\n",
    "            cut, add = text_change.split(' => ')\n",
    "            text_recons = text_recons[:curs_pos - len(add)] + add + text_recons[curs_pos - len(add) + len(cut):]\n",
    "\n",
    "        if \"Move\" in activity:\n",
    "            a, b, c, d = map(\n",
    "                        int,\n",
    "                        re.match(\n",
    "                            r\"Move From \\[(\\d+), (\\d+)\\] To \\[(\\d+), (\\d+)\\]\",\n",
    "                            activity,\n",
    "                        ).groups(),\n",
    "                    )\n",
    "\n",
    "            if a != c:\n",
    "                if a < c:\n",
    "                    text_recons = text_recons[:a] + text_recons[b:d] + text_recons[a:b] + text_recons[d:]\n",
    "                else:\n",
    "                    text_recons = text_recons[:c] + text_recons[a:b] + text_recons[c:a] + text_recons[b:]\n",
    "                    \n",
    "    return text_recons\n",
    "\n",
    "\n",
    "def get_text_start_end(txt, s, search_from=0):\n",
    "    txt = txt[int(search_from):]\n",
    "    try:\n",
    "        idx = txt.find(s)\n",
    "        if idx >= 0:\n",
    "            st = idx\n",
    "            ed = st + len(s)\n",
    "        else:\n",
    "            raise ValueError('Error')\n",
    "    except:\n",
    "        res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "        if len(res):\n",
    "            st, ed = res[0][0], res[0][1]\n",
    "        else:\n",
    "            m = SequenceMatcher(None, s, txt).get_opcodes()\n",
    "            for tag, i1, i2, j1, j2 in m:\n",
    "                if tag == 'replace':\n",
    "                    s = s[:i1] + txt[j1:j2] + s[i2:]\n",
    "                if tag == \"delete\":\n",
    "                    s = s[:i1] + s[i2:]\n",
    "\n",
    "            res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "            if len(res):\n",
    "                st, ed = res[0][0], res[0][1]\n",
    "            else:\n",
    "                idx = txt.find(s)\n",
    "                if idx >= 0:\n",
    "                    st = idx\n",
    "                    ed = st + len(s)\n",
    "                else:\n",
    "                    st, ed = 0, 0\n",
    "    return st + search_from, ed + search_from\n",
    "\n",
    "\n",
    "def get_offset_mapping(full_text, tokens):\n",
    "    offset_mapping = []\n",
    "\n",
    "    current_offset = 0\n",
    "    for token in tokens:\n",
    "        start, end = get_text_start_end(full_text, token, search_from=current_offset)\n",
    "        offset_mapping.append((start, end))\n",
    "        current_offset = end\n",
    "\n",
    "    return offset_mapping\n",
    "\n",
    "def get_start_end(col):\n",
    "    def search_start_end(row):\n",
    "        txt = row.essay_text\n",
    "        search_from = row.previous_discourse_end\n",
    "        s = row[col]\n",
    "        # print(search_from)\n",
    "        return get_text_start_end(txt,s,search_from)\n",
    "    return search_start_end\n",
    "\n",
    "def batch_to_device(batch, device):\n",
    "    batch_dict = {key: batch[key].to(device) for key in batch}\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "def text_to_words(text):\n",
    "    word = text.split()\n",
    "    word_offset = []\n",
    "\n",
    "    start = 0\n",
    "    for w in word:\n",
    "        r = text[start:].find(w)\n",
    "\n",
    "        if r==-1:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            start = start+r\n",
    "            end   = start+len(w)\n",
    "            word_offset.append((start,end))\n",
    "        start = end\n",
    "\n",
    "    return word, word_offset\n",
    "\n",
    "def text_to_sentence(text):\n",
    "    sentences = re.split(r' *[\\.\\?!\\n][\\'\"\\)\\]]* *', text)\n",
    "    sentences = [x for x in sentences if x!=\"\"]\n",
    "    \n",
    "    sentence_offset = []\n",
    "    start = 0\n",
    "    for w in sentences:\n",
    "        r = text[start:].find(w)\n",
    "\n",
    "        if r==-1:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            start = start+r\n",
    "            end   = start+len(w)\n",
    "            sentence_offset.append((start,end))\n",
    "        start = end\n",
    "\n",
    "    return sentences,sentence_offset\n",
    "\n",
    "def text_to_paragraph(text):\n",
    "    sentences = re.split(r' *[\\n][\\'\"\\)\\]]* *', text)\n",
    "    sentences = [x for x in sentences if x!=\"\"]\n",
    "    \n",
    "    sentence_offset = []\n",
    "    start = 0\n",
    "    for w in sentences:\n",
    "        r = text[start:].find(w)\n",
    "\n",
    "        if r==-1:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            start = start+r\n",
    "            end   = start+len(w)\n",
    "            sentence_offset.append((start,end))\n",
    "        start = end\n",
    "\n",
    "    return sentences,sentence_offset\n",
    "\n",
    "\n",
    "def get_span_from_text(text,span_type=\"words\"):\n",
    "    \n",
    "    if span_type==\"words\":\n",
    "        spans,spans_offset = text_to_words(text)\n",
    "    elif span_type==\"sentences\":\n",
    "        spans,spans_offset = text_to_sentence(text)\n",
    "    else:\n",
    "        spans,spans_offset = text_to_paragraph(text)\n",
    "    \n",
    "    return spans,spans_offset\n",
    "\n",
    "\n",
    "def get_span_len_from_text(text,span_type=\"words\"):\n",
    "    \n",
    "    if span_type==\"words\":\n",
    "        spans_len = len(text.split())\n",
    "    elif span_type==\"sentences\":\n",
    "        sentences = re.split(r' *[\\.\\?!\\n][\\'\"\\)\\]]* *', text)\n",
    "        spans_len = len([x for x in sentences if x!=\"\"])\n",
    "    else:\n",
    "        sentences = re.split(r' *[\\n][\\'\"\\)\\]]* *', text)\n",
    "        spans_len = len([x for x in sentences if x!=\"\"])\n",
    "    \n",
    "    return spans_len\n",
    "\n",
    "\n",
    "def to_gpu(data,device):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: to_gpu(v,device) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_gpu(v,device) for v in data]\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "def to_np(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: to_np(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_np(v) for v in data]\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.cpu().numpy()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def get_start_end_offset(col):\n",
    "    def search_start_end(row):\n",
    "        txt = row.full_text\n",
    "        toks = row[col]\n",
    "        # print(search_from)\n",
    "        return get_offset_mapping(txt,toks)\n",
    "    return search_start_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "209ca00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "# from data.data_utils import clean_text,get_start_end,get_offset_mapping,get_start_end_offset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "LABEL2TYPE = ('NAME_STUDENT','EMAIL','USERNAME','ID_NUM', 'PHONE_NUM','URL_PERSONAL','STREET_ADDRESS','O')\n",
    "TYPE2LABEL = {t: l for l, t in enumerate(LABEL2TYPE)}\n",
    "\n",
    "\n",
    "## =============================================================================== ##\n",
    "class FeedbackDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 tokenizer,\n",
    "                 mask_prob=0.0,\n",
    "                 mask_ratio=0.0,\n",
    "                 train = True\n",
    "                 ):\n",
    "        \n",
    "        self.train = True\n",
    "        self.tokenizer = tokenizer\n",
    "        if len(self.tokenizer.encode(\"\\n\\n\"))==2:\n",
    "            df[\"full_text\"] = df['full_text'].transform(lambda x:x.str.replace(\"\\n\\n\",\" | \"))\n",
    "            df[\"tokens\"] = df['tokens'].transform(lambda x:[i.str.replace(\"\\n\\n\",\" | \") for i in x])\n",
    "\n",
    "        self.df = self.prepare_df(df)\n",
    "\n",
    "        print(f'Loaded {len(self)} samples.')\n",
    "\n",
    "        assert 0 <= mask_prob <= 1\n",
    "        assert 0 <= mask_ratio <= 1\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        df = self.df.iloc[index]\n",
    "        text = df['full_text']\n",
    "        text_id = df['document']\n",
    "        labels = df['labels'] if self.train else [] \n",
    "        \n",
    "        tokens = self.tokenizer(text, return_offsets_mapping=True)\n",
    "        input_ids = torch.LongTensor(tokens['input_ids'])\n",
    "        attention_mask = torch.LongTensor(tokens['attention_mask'])\n",
    "        offset_mapping = np.array(tokens['offset_mapping'])\n",
    "#         offset_mapping = self.strip_offset_mapping(text, offset_mapping)\n",
    "        num_tokens = len(input_ids)\n",
    "\n",
    "        # token slices of words\n",
    "        woff = np.array(df['offset'])\n",
    "        toff = offset_mapping\n",
    "        wx1, wx2 = woff.T\n",
    "        tx1, tx2 = toff.T\n",
    "        ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n",
    "        ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n",
    "        ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n",
    "        ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n",
    "        ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1)\n",
    "#         assert (ious > 0).any(-1).all()\n",
    "\n",
    "        word_boxes = []\n",
    "#         err = []\n",
    "        for i,row in enumerate(ious):\n",
    "            inds = row.nonzero()[0]\n",
    "            try:\n",
    "                word_boxes.append([inds[0], 0, inds[-1] + 1, 1])\n",
    "            except:\n",
    "                word_boxes.append([-100, 0, -99, 1])\n",
    "#                 err.append(i)\n",
    "                \n",
    "        word_boxes = torch.FloatTensor(word_boxes)\n",
    "\n",
    "        # word slices of ground truth spans\n",
    "        gt_spans = []        \n",
    "        for i,label in enumerate(labels) :\n",
    "#             if i not in err:\n",
    "            gt_spans.append([i,TYPE2LABEL[label.split('-')[1] if label!=\"O\" else \"O\"]])\n",
    "            \n",
    "        gt_spans = torch.LongTensor(gt_spans)\n",
    "\n",
    "        # random mask augmentation\n",
    "        if np.random.random() < self.mask_prob:\n",
    "            all_inds = np.arange(1, len(input_ids) - 1)\n",
    "            n_mask = max(int(len(all_inds) * self.mask_ratio), 1)\n",
    "            np.random.shuffle(all_inds)\n",
    "            mask_inds = all_inds[:n_mask]\n",
    "            input_ids[mask_inds] = self.tokenizer.mask_token_id\n",
    "\n",
    "        return dict(text=text,\n",
    "                    text_id=text_id,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    word_boxes=word_boxes,\n",
    "                    gt_spans=gt_spans)\n",
    "    \n",
    "    def prepare_df(self,test_df):\n",
    "        test_df['full_text'] = test_df['full_text'].transform(clean_text)        \n",
    "        test_df['tokens'] = test_df['tokens'].transform(lambda x:[clean_text(i) for i in x])\n",
    "        test_df['offset'] = test_df.apply(get_start_end_offset('tokens'),axis=1)\n",
    "#         test_df['nb_labels'] = test_df['labels'].transform(lambda x:len([i for i in x if i!=\"O\" ]))\n",
    "        return test_df\n",
    "    \n",
    "    def strip_offset_mapping(self, text, offset_mapping):\n",
    "        ret = []\n",
    "        for start, end in offset_mapping:\n",
    "            match = list(re.finditer('\\S+', text[start:end]))\n",
    "            if len(match) == 0:\n",
    "                ret.append((start, end))\n",
    "            else:\n",
    "                span_start, span_end = match[0].span()\n",
    "                ret.append((start + span_start, start + span_end))\n",
    "        return np.array(ret)\n",
    "\n",
    "    def get_word_offsets(self, text):\n",
    "        matches = re.finditer(\"\\S+\", text)\n",
    "        spans = []\n",
    "        words = []\n",
    "        for match in matches:\n",
    "            span = match.span()\n",
    "            word = match.group()\n",
    "            spans.append(span)\n",
    "            words.append(word)\n",
    "        assert tuple(words) == tuple(text.split())\n",
    "        return np.array(spans)\n",
    "    \n",
    "## =============================================================================== ##\n",
    "class CustomCollator(object):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        if hasattr(model.config, 'attention_window'):\n",
    "            # For longformer\n",
    "            # https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/longformer/modeling_longformer.py#L1548\n",
    "            self.attention_window = (model.config.attention_window\n",
    "                                     if isinstance(\n",
    "                                         model.config.attention_window, int)\n",
    "                                     else max(model.config.attention_window))\n",
    "        else:\n",
    "            self.attention_window = None\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        batch_size = len(samples)\n",
    "        assert batch_size == 1, f'Only batch_size=1 supported, got batch_size={batch_size}.'\n",
    "\n",
    "        sample = samples[0]\n",
    "\n",
    "        max_seq_length = len(sample['input_ids'])\n",
    "        if self.attention_window is not None:\n",
    "            attention_window = self.attention_window\n",
    "            padded_length = (attention_window -\n",
    "                             max_seq_length % attention_window\n",
    "                             ) % attention_window + max_seq_length\n",
    "        else:\n",
    "            padded_length = max_seq_length\n",
    "\n",
    "        input_shape = (1, padded_length)\n",
    "        input_ids = torch.full(input_shape,\n",
    "                               self.pad_token_id,\n",
    "                               dtype=torch.long)\n",
    "        attention_mask = torch.zeros(input_shape, dtype=torch.long)\n",
    "\n",
    "        seq_length = len(sample['input_ids'])\n",
    "        input_ids[0, :seq_length] = sample['input_ids']\n",
    "        attention_mask[0, :seq_length] = sample['attention_mask']\n",
    "\n",
    "        text_id = sample['text_id']\n",
    "        text = sample['text']\n",
    "        word_boxes = sample['word_boxes']\n",
    "        gt_spans = sample['gt_spans']\n",
    "\n",
    "        return dict(text_id=text_id,\n",
    "                    text=text,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    word_boxes=word_boxes,\n",
    "                    gt_spans=gt_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0171a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d29aa06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "model_name = 'microsoft/deberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba1b4fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6807 samples.\n"
     ]
    }
   ],
   "source": [
    "ds = FeedbackDataset(df,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed190fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (867 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dx = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d3202e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dx[\"word_boxes\"]!=-100)[:,0]*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f9f69db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([753, 2])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx[\"gt_spans\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0827d2",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "255fae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel\n",
    "import torch.utils.checkpoint\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "# from mmcv.cnn import bias_init_with_prob\n",
    "from torchvision.ops import roi_align, nms\n",
    "\n",
    "def aggregate_tokens_to_words(feat, word_boxes):\n",
    "    feat = feat.permute(0, 2, 1).unsqueeze(2)\n",
    "    output = roi_align(feat, [word_boxes], 1, aligned=True)\n",
    "    return output.squeeze(-1).squeeze(-1)\n",
    "\n",
    "\n",
    "def span_nms(start, end, score, nms_thr=0.5):\n",
    "    boxes = torch.stack(\n",
    "        [\n",
    "            start,\n",
    "            torch.zeros_like(start),\n",
    "            end,\n",
    "            torch.ones_like(start),\n",
    "        ],\n",
    "        dim=1,\n",
    "    ).float()\n",
    "    keep = nms(boxes, score, nms_thr)\n",
    "    return keep\n",
    "\n",
    "class FeedbackModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name,\n",
    "                 num_labels = 8,\n",
    "                 config_path=None,\n",
    "                 pretrained_path = None,\n",
    "                 use_dropout=False,\n",
    "                 use_gradient_checkpointing = False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.config = AutoConfig.from_pretrained(model_name, output_hidden_states=True) if not config_path else torch.load(config_path)\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "        if not self.use_dropout:\n",
    "            self.config.update(\n",
    "                                {\n",
    "                                    \"hidden_dropout_prob\": 0.0,\n",
    "                                    \"attention_probs_dropout_prob\": 0.0,\n",
    "                                }\n",
    "                                    )\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(model_name,config=self.config) if not config_path else AutoModel.from_config(self.config)        \n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        \n",
    "        if self.pretrained_path:\n",
    "            try:\n",
    "                self.load_from_cp()\n",
    "            except:\n",
    "                pass\n",
    "        if use_gradient_checkpointing:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "        # self.fc.bias.data[0].fill_(bias_init_with_prob(0.02))\n",
    "        # self.fc.bias.data[3:-3].fill_(bias_init_with_prob(1 / num_label_discourse_type))\n",
    "        # self.fc.bias.data[-3:].fill_(bias_init_with_prob(1 / num_label_effectiveness))\n",
    "\n",
    "    def load_from_cp(self):\n",
    "        print(\"Using Pretrained Weights\")\n",
    "        print(self.pretrained_path)\n",
    "        state_dict = torch.load(self.pretrained_path, map_location=lambda storage, loc: storage)\n",
    "        del state_dict['fc.bias']\n",
    "        del state_dict['fc.weight']\n",
    "\n",
    "        if 'fc_seg.bias' in state_dict.keys():\n",
    "            del state_dict['fc_seg.bias']\n",
    "            del state_dict['fc_seg.weight']\n",
    "            for key in list(state_dict.keys()):\n",
    "                state_dict[key.replace('model.deberta.', '')] = state_dict.pop(key)\n",
    "        else:\n",
    "            for key in list(state_dict.keys()):\n",
    "                state_dict[key.replace('backbone.', '')] = state_dict.pop(key)\n",
    "         \n",
    "        self.backbone.load_state_dict(state_dict, strict=True)\n",
    "        print('Loading successed !')\n",
    "\n",
    "    def forward(self,b):\n",
    "        x = self.backbone(b[\"input_ids\"],b[\"attention_mask\"]).last_hidden_state\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = aggregate_tokens_to_words(x, b['word_boxes'])\n",
    "        # obj_pred = x[..., 0]\n",
    "        # reg_pred = x[..., 1:3]\n",
    "        # type_pred = x[..., 3:-3]\n",
    "        # eff_pred = x[..., -3:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "163be7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = FeedbackModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d1a723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b027d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = CustomCollator(tokenizer,model)\n",
    "train_loader = DataLoader(ds,batch_size=1,collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b2a1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d524d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a0e539ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = y.softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f0352cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s,i = yy.max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1f3b1cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 2, 3, 2, 1, 6, 1, 2, 1, 1, 1, 0, 5, 6, 3, 0, 2, 3, 3, 3, 2, 3, 0, 1,\n",
       "        3, 1, 3, 3, 2, 0, 2, 2, 5, 4, 5, 0, 3, 0, 2, 0, 2, 3, 1, 1, 3, 0, 2, 3,\n",
       "        3, 6, 1, 2, 6, 1, 6, 6, 1, 5, 5, 2, 1, 1, 6, 0, 5, 1, 5, 1, 5, 5, 5, 5,\n",
       "        5, 5, 5, 3, 0, 2, 6, 6, 5, 6, 1, 1, 1, 0, 1, 5, 6, 3, 6, 5, 0, 2, 2, 3,\n",
       "        6, 6, 2, 4, 3, 0, 6, 2, 6, 3, 2, 5, 6, 0, 6, 1, 6, 2, 6, 6, 0, 6, 0, 6,\n",
       "        6, 5, 6, 4, 6, 6, 6, 6, 5, 5, 5, 6, 6, 6, 6, 0, 6, 0, 6, 0, 6, 6, 5, 0,\n",
       "        6, 0, 6, 6, 1, 6, 2, 2, 6, 0, 6, 0, 6, 6, 6, 1, 4, 6, 6, 6, 0, 6, 2, 2,\n",
       "        2, 2, 2, 1, 2, 6, 2, 0, 1, 2, 0, 6, 0, 6, 6, 6, 3, 4, 5, 6, 6, 6, 6, 2,\n",
       "        0, 6, 0, 6, 6, 5, 6, 6, 5, 0, 6, 0, 6, 6, 3, 2, 2, 5, 5, 2, 3, 6, 6, 0,\n",
       "        6, 0, 6, 1, 0, 6, 0, 6, 6, 2, 5, 5, 0, 6, 0, 6, 6, 5, 5, 5, 2, 0, 1, 1,\n",
       "        5, 2, 3, 1, 1, 1, 1, 2, 6, 2, 5, 1, 1, 1, 2, 2, 2, 1, 0, 2, 2, 5, 5, 0,\n",
       "        2, 2, 2, 2, 2, 1, 2, 2, 2, 3, 2, 0, 2, 2, 0, 6, 0, 7, 2, 0, 2, 2, 2, 2,\n",
       "        0, 1, 4, 5, 2, 1, 2, 2, 3, 5, 6, 6, 6, 6, 2, 2, 2, 2, 2, 0, 6, 2, 5, 3,\n",
       "        1, 1, 5, 1, 2, 2, 2, 0, 1, 5, 5, 0, 6, 1, 1, 2, 5, 2, 2, 0, 2, 1, 2, 2,\n",
       "        4, 4, 4, 2, 1, 0, 2, 2, 2, 1, 1, 2, 2, 0, 2, 2, 0, 3, 1, 2, 4, 4, 6, 4,\n",
       "        1, 2, 2, 3, 2, 0, 2, 2, 2, 3, 2, 4, 4, 4, 2, 0, 4, 0, 0, 3, 1, 2, 3, 1,\n",
       "        2, 1, 0, 2, 2, 3, 1, 6, 2, 2, 0, 3, 2, 4, 3, 3, 3, 3, 2, 2, 4, 1, 2, 2,\n",
       "        1, 2, 1, 0, 1, 3, 2, 2, 2, 3, 2, 1, 2, 3, 2, 0, 2, 1, 1, 2, 6, 2, 6, 3,\n",
       "        2, 4, 3, 2, 2, 2, 2, 0, 0, 4, 1, 2, 2, 5, 0, 5, 5, 4, 2, 6, 2, 2, 2, 2,\n",
       "        2, 2, 4, 2, 3, 3, 2, 2, 4, 1, 0, 3, 5, 2, 2, 1, 0, 5, 2, 3, 2, 2, 5, 3,\n",
       "        2, 5, 2, 1, 0, 2, 1, 2, 2, 1, 6, 0, 2, 2, 4, 0, 2, 0, 1, 2, 2, 0, 1, 0,\n",
       "        2, 2, 2, 6, 3, 3, 1, 3, 2, 2, 0, 2, 0, 2, 2, 6, 4, 0, 2, 3, 2, 5, 2, 2,\n",
       "        0, 3, 2, 2, 4, 1, 1, 1, 5, 2, 0, 5, 0, 2, 2, 2, 3, 1, 2, 0, 6, 3, 1, 2,\n",
       "        2, 3, 3, 2, 2, 3, 5, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 1, 2, 2, 3, 2, 3, 3,\n",
       "        0, 0, 4, 2, 2, 3, 0, 2, 3, 2, 2, 0, 5, 5, 4, 4, 2, 5, 4, 4, 1, 1, 2, 2,\n",
       "        1, 3, 2, 3, 4, 5, 2, 3, 0, 2, 2, 0, 0, 2, 2, 5, 6, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 5, 0, 0, 3, 3, 4, 2, 3, 2, 5, 2, 6, 6, 6, 0, 2, 3, 2, 2, 2, 0,\n",
       "        4, 3, 2, 3, 2, 0, 6, 6, 2, 4, 1, 2, 0, 5, 0, 5, 5, 3, 5, 1, 2, 2, 2, 1,\n",
       "        2, 2, 2, 4, 5, 0, 4, 4, 0, 2, 2, 2, 0, 1, 6, 6, 6, 1, 2, 2, 1, 5, 1, 2,\n",
       "        6, 5, 2, 0, 0, 0, 6, 2, 6, 6, 5, 3, 5, 5, 2, 2, 2, 3, 2, 2, 0, 0, 5, 6,\n",
       "        2, 2, 0, 6, 6, 2, 2, 2, 2, 5, 6, 0, 5, 2, 3, 2, 2, 5, 5, 2, 5, 2, 1, 0,\n",
       "        6, 1, 1, 3, 1, 5, 5, 5, 0])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7bed5a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([753, 8])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f63f7cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NAME_STUDENT': 0,\n",
       " 'EMAIL': 1,\n",
       " 'USERNAME': 2,\n",
       " 'ID_NUM': 3,\n",
       " 'PHONE_NUM': 4,\n",
       " 'URL_PERSONAL': 5,\n",
       " 'STREET_ADDRESS': 6,\n",
       " 'O': 7}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TYPE2LABEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
