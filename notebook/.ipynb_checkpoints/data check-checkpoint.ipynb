{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d487dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Script/NLP/PII/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84bf958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1602e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_utils import get_offset_mapping,clean_text\n",
    "from data.dataset import FeedbackDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7384a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa9ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pylab import cm, matplotlib\n",
    "import os\n",
    "\n",
    "colors = {\n",
    "            'NAME_STUDENT': '#8000ff',\n",
    "            'EMAIL': '#2b7ff6',\n",
    "            'USERNAME': '#2adddd',\n",
    "            'ID_NUM': '#80ffb4',\n",
    "            'PHONE_NUM': 'd4dd80',\n",
    "            'URL_PERSONAL': '#ff8042',\n",
    "            'STREET_ADDRESS': '#ff0000'\n",
    "         }\n",
    "\n",
    "\n",
    "def visualize(full_text,offset_mapping,labels):\n",
    "    \n",
    "    ents = []\n",
    "    for offset,lab in zip(offset_mapping,labels):\n",
    "        ents.append({\n",
    "                        'start': int(offset[0]), \n",
    "                         'end': int(offset[1]), \n",
    "                         'label': str(lab.split('-')[-1]) #+ ' - ' + str(row['discourse_effectiveness'])\n",
    "                    })\n",
    "\n",
    "    doc2 = {\n",
    "        \"text\": full_text,\n",
    "        \"ents\": ents,\n",
    "#         \"title\": \"idx\"\n",
    "    }\n",
    "\n",
    "    options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895aef6",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0104cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.json',\n",
       " 'pii_dataset_fixed.csv',\n",
       " 'mixtral-8x7b-v1.json',\n",
       " '.~lock.lecture2.pptx#',\n",
       " 'mpware_mixtral8x7b_v1.1-no-i-username.json.zip',\n",
       " 'Fake_data_1850_218.json',\n",
       " 'test.json',\n",
       " 'archive.zip',\n",
       " 'archive',\n",
       " 'pii-masking-200k.csv',\n",
       " 'moredata_dataset_fixed.csv',\n",
       " 'sample_submission.csv',\n",
       " 'mpware_mixtral8x7b_v1.1.json']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(r\"/database/kaggle/PII/data\")\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73fc670f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.read_csv(data_path/'sample_submission.csv')\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced5d03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>482</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>483</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>741</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  document  token           label\n",
       "0       0         7      9  B-NAME_STUDENT\n",
       "1       1         7     10  I-NAME_STUDENT\n",
       "2       2         7    482  B-NAME_STUDENT\n",
       "3       3         7    483  I-NAME_STUDENT\n",
       "4       4         7    741  B-NAME_STUDENT"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80aa459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-NAME_STUDENT', 'I-NAME_STUDENT'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc125ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(data_path/'train.json')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "777aa2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text                                             tokens                                trailing_whitespace                                             labels\n",
       "0         7  Design Thinking for innovation reflexion-Avril...  [Design, Thinking, for, innovation, reflexion,...  [True, True, True, True, False, False, True, F...  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...\n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...  [True, False, False, True, True, False, False,...  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc7b295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL2TYPE = ('NAME_STUDENT','EMAIL','USERNAME','ID_NUM', 'PHONE_NUM','URL_PERSONAL','STREET_ADDRESS','O')\n",
    "len(LABEL2TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "400ffd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in LABEL2TYPE[:-1]:\n",
    "    df[name] = ((df['labels'].transform(lambda x:len([i for i in x if i.split('-')[-1]==name ])>0)))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6aa3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nb_labels'] = df['labels'].transform(lambda x:len([i for i in x if i!=\"O\" ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38e76e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5862\n",
       "2      599\n",
       "4      108\n",
       "1       86\n",
       "3       52\n",
       "6       46\n",
       "8       14\n",
       "5       10\n",
       "12       6\n",
       "10       5\n",
       "11       3\n",
       "9        3\n",
       "15       2\n",
       "14       2\n",
       "21       2\n",
       "7        1\n",
       "23       1\n",
       "18       1\n",
       "17       1\n",
       "26       1\n",
       "34       1\n",
       "22       1\n",
       "Name: nb_labels, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nb_labels'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a59c9509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_STUDENT       891\n",
       "EMAIL               24\n",
       "USERNAME             5\n",
       "ID_NUM              33\n",
       "PHONE_NUM            4\n",
       "URL_PERSONAL        72\n",
       "STREET_ADDRESS       2\n",
       "nb_labels         2739\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[list(LABEL2TYPE)[:-1]+['nb_labels']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db5e835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417a4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42]\n",
    "folds_names = []\n",
    "for K in [5]:  \n",
    "    for seed in seeds:\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=K,shuffle=True,random_state=seed)\n",
    "        name = f\"fold_msk_{K}_seed_{seed}\"\n",
    "        df[name] = -1\n",
    "        for fold, (trn_, val_) in enumerate(mskf.split(df,df[list(LABEL2TYPE)[:-1]])):\n",
    "            df.loc[val_, name] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a92d9cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_STUDENT</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>ID_NUM</th>\n",
       "      <th>PHONE_NUM</th>\n",
       "      <th>URL_PERSONAL</th>\n",
       "      <th>STREET_ADDRESS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold_msk_5_seed_42</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    NAME_STUDENT  EMAIL  USERNAME  ID_NUM  PHONE_NUM  URL_PERSONAL  STREET_ADDRESS\n",
       "fold_msk_5_seed_42                                                                                \n",
       "0                            178      5         1       7          1            14               0\n",
       "1                            178      5         1       7          1            14               0\n",
       "2                            179      5         1       6          1            15               1\n",
       "3                            178      4         1       6          0            15               1\n",
       "4                            178      5         1       7          1            14               0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(name)[list(LABEL2TYPE)[:-1]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6525316c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09279438461710321"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c570c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from text_unidecode import unidecode\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "\n",
    "try:\n",
    "    from faker import Faker\n",
    "    fake = Faker()\n",
    "    Faker.seed(0)\n",
    "except:\n",
    "    print('No faker installed')\n",
    "    \n",
    "try:\n",
    "    from spacy.lang.en import English\n",
    "    EN_TOK = English().tokenizer\n",
    "except:\n",
    "    print(\"No spacy\")\n",
    "\n",
    "    \n",
    "    \n",
    "def process_regex(pattern, reverse=False):\n",
    "    replacements = {\n",
    "        '(': r'\\(',\n",
    "        ')': r'\\)',\n",
    "        '[': r'\\[',\n",
    "        ']': r'\\]',\n",
    "        '|': r'\\|',\n",
    "        '?': r'\\?',\n",
    "        '*': r'\\*',\n",
    "        '+': r'\\+'\n",
    "    }\n",
    "    \n",
    "    if reverse:\n",
    "        replacements = {v: k for k, v in replacements.items()}\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        pattern = pattern.replace(old, new)\n",
    "    \n",
    "    return pattern\n",
    "\n",
    "\n",
    "\n",
    "LABEL2TYPE = ('NAME_STUDENT','EMAIL','USERNAME','ID_NUM', 'PHONE_NUM','URL_PERSONAL','STREET_ADDRESS','O')\n",
    "TYPE2LABEL = {t: l for l, t in enumerate(LABEL2TYPE)}\n",
    "ID_TYPE = {\"0-0\":0,\"0-1\":1,\n",
    "           \"1-0\":2,\"1-1\":3,\n",
    "           \"2-0\":4,\"2-1\":5,\n",
    "           \"3-0\":6,\"3-1\":7,\n",
    "           \"4-0\":8,\"4-1\":9,\n",
    "           \"5-0\":10,\"5-1\":11,\n",
    "           \"6-0\":12,\"6-1\":13\n",
    "          }\n",
    "\n",
    "ID_NAME = {\"0-0\":\"B-NAME_STUDENT\",\"0-1\":\"I-NAME_STUDENT\",\n",
    "           \"1-0\":\"B-EMAIL\",\"1-1\":\"I-EMAIL\",\n",
    "           \"2-0\":\"B-USERNAME\",\"2-1\":\"I-USERNAME\",\n",
    "           \"3-0\":\"B-ID_NUM\",\"3-1\":\"I-ID_NUM\",\n",
    "           \"4-0\":\"B-PHONE_NUM\",\"4-1\":\"I-PHONE_NUM\",\n",
    "           \"5-0\":\"B-URL_PERSONAL\",\"5-1\":\"I-URL_PERSONAL\",\n",
    "           \"6-0\":\"B-STREET_ADDRESS\",\"6-1\":\"I-STREET_ADDRESS\",\n",
    "           \"7-0\":\"O\",\"7-1\":\"O\"\n",
    "          }\n",
    "\n",
    "RE_ID_PHONE = r\"\"\"(\\(?\\+\\s*\\d{1,4}\\s*\\)?\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\s{0,2}\\d{0,5}|\\(?\\+\\s*\\d{1,4}\\s*\\)?\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s{0,2}\\d{0,5}|\\(?\\s*\\d{1,4}\\s*\\)?\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s{0,2}\\d{0,5}\\s*[\\.\\-x]?\\d{1,5}\\s{0,2}\\d{0,5}|\\(?\\s*\\d{1,4}\\s*\\)?\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s{0,2}\\d{0,5}|\\(\\s*\\d{3}\\s*\\)\\s*\\d{3}\\s*\\-\\s*\\d{4}\\s*\\w{0,3}(\\s*\\d{1,8}\\s*)?|\\b\\d{2,}-\\d{2,}\\.\\d{2,}\\.\\d{2,}\\.\\d{2,}\\b|\\b\\d{2,}-\\d{2,}\\-\\d{2,}\\-\\d{2,}\\-\\d{2,}\\b|\\b\\d{2,}\\-\\d{2,}\\-\\d{2,}\\-\\d{2,}\\b|\\b\\d{2,}\\.\\d{2,}\\.\\d{2,}\\.\\d{2,}\\b|\\d{3}\\s*\\.\\s*\\d{3}\\s*\\.\\s*\\d{1,5}|\\d{3}\\s*\\-\\s*\\d{3}\\s*\\-\\s*\\d{1,5}|\\d{3}\\s*x\\s*\\d{3}\\s*x\\s*\\d{1,5}|\\d{1,3}\\s{0,2}\\d{1,}\\s{0,2}\\d{1,}|\\b\\d{1,}\\s*\\d{1,}\\s*\\d{1,}|\\b\\d{2,}\\-\\d{2,}\\-\\d{2,}\\b|\\b\\d{2,}\\.\\d{2,}\\.\\d{2,}\\b|\\b\\d{1,}-\\d{1,}|[\\w\\.\\:\\-\\_\\|]*\\d{6,})\"\"\"\n",
    "REGEX_COMPILE = re.compile(RE_ID_PHONE)\n",
    "\n",
    "\n",
    "## =============================================================================== ##\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 tokenizer,\n",
    "                 add_text_prob=0.9,\n",
    "                 replace_text_prob=0.5,\n",
    "                 attrib_to_replace = ['NAME_STUDENT','EMAIL','USERNAME','ID_NUM',\n",
    "                                      'PHONE_NUM','URL_PERSONAL','STREET_ADDRESS']\n",
    "                 ):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.attrib_to_replace = attrib_to_replace\n",
    "\n",
    "        print(f'Loaded {len(self)} samples.')\n",
    "\n",
    "        assert 0 <= add_text_prob <= 1\n",
    "        assert 0 <= replace_text_prob <= 1\n",
    "        self.add_text_prob = add_text_prob\n",
    "        self.replace_text_prob = replace_text_prob\n",
    "    # ======================================================================================== #\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    # ======================================================================================== #\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        df = self.df.iloc[index]\n",
    "        text_id = df['document']\n",
    "        \n",
    "        ## Adding space tokens\n",
    "        if len(self.tokenizer.encode(\"\\n\\n\"))==2:\n",
    "            text = self.clean_text(df['full_text'].replace(\"\\n\\n\",\" | \").replace(\"\\n\",\" [BR] \"))\n",
    "            txt_tokens = [self.clean_text(x.replace(\"\\n\\n\",\" | \").replace(\"\\n\",\" [BR] \")) for x in df['tokens']]\n",
    "        else:\n",
    "            text = self.clean_text(df['full_text'])\n",
    "            txt_tokens = [self.clean_text(x) for x in df['tokens']]\n",
    "            \n",
    "        labels = df['labels']\n",
    "        offset_mapping_init = self.get_offset_mapping(text,txt_tokens)\n",
    "        \n",
    "        ##############   Augmentation   #############\n",
    "        # Replace PII\n",
    "#         prob = \n",
    "        if np.random.random()< self.replace_text_prob:\n",
    "            print(\"rep\")\n",
    "            text,labels,txt_tokens = self.create_mapper_n_clean(text,labels,offset_mapping_init,\n",
    "                                                                       attribut=self.attrib_to_replace)\n",
    "            offset_mapping_init = self.get_offset_mapping(text, txt_tokens)\n",
    "        \n",
    "\n",
    "    \n",
    "        # Add PII\n",
    "        if np.random.random() < self.add_text_prob:\n",
    "            print(\"add\")\n",
    "            new_text,new_tokens,new_labels,new_offset_mapping = self.generate_fake_data()\n",
    "            \n",
    "        \n",
    "            \n",
    "            text,txt_tokens,labels,offset_mapping_init  = self.add_text(text,txt_tokens,labels,offset_mapping_init,\n",
    "                                                                   new_text,new_tokens,new_labels,new_offset_mapping)\n",
    "        \n",
    "        ### Convert to regex space\n",
    "        re_offset_mapping,spacy_to_re,re_tokens = self.spacy_to_re_off(text,txt_tokens,offset_mapping_init)\n",
    "\n",
    "\n",
    "        hf_tokens = self.tokenizer(text, return_offsets_mapping=True)\n",
    "        input_ids = torch.LongTensor(hf_tokens['input_ids'])\n",
    "        attention_mask = torch.LongTensor(hf_tokens['attention_mask'])\n",
    "        hf_offset_mapping = np.array(hf_tokens['offset_mapping'])\n",
    "\n",
    "        num_tokens = len(input_ids)\n",
    "\n",
    "        # token slices of words\n",
    "        woff = np.array(re_offset_mapping)\n",
    "        toff = np.array(hf_offset_mapping)\n",
    "        wx1, wx2 = woff.T\n",
    "        tx1, tx2 = toff.T\n",
    "        ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n",
    "        ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n",
    "        ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n",
    "        ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n",
    "        ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1)\n",
    "#         assert (ious > 0).any(-1).all()\n",
    "\n",
    "        word_boxes = []\n",
    "#         err = []\n",
    "        for i,row in enumerate(ious):\n",
    "            inds = row.nonzero()[0]\n",
    "            try:\n",
    "                word_boxes.append([inds[0], 0, inds[-1] + 1, 1])\n",
    "            except:\n",
    "                word_boxes.append([-100, 0, -99, 1])\n",
    "#                 err.append(i)\n",
    "                \n",
    "        word_boxes = torch.FloatTensor(word_boxes)\n",
    "\n",
    "\n",
    "        gt_spans = []        \n",
    "        for i,label in enumerate(labels) :\n",
    "            gt_spans.append([i,TYPE2LABEL[label.split('-')[-1] if label!=\"O\" else \"O\"],0 if label.split('-')[0]==\"B\" else 1])\n",
    "            \n",
    "        gt_spans = torch.LongTensor(gt_spans)\n",
    "\n",
    "        # random mask augmentation\n",
    "#         if np.random.random() < self.mask_prob:\n",
    "#             all_inds = np.arange(1, len(input_ids) - 1)\n",
    "#             n_mask = max(int(len(all_inds) * self.mask_ratio), 1)\n",
    "#             np.random.shuffle(all_inds)\n",
    "#             mask_inds = all_inds[:n_mask]\n",
    "#             input_ids[mask_inds] = self.tokenizer.mask_token_id\n",
    "\n",
    "        return dict(\n",
    "                    text_id=text_id,\n",
    "                    text=text,\n",
    "                    labels = labels,\n",
    "                    re_tokens = re_tokens,\n",
    "                    spacy_to_re = spacy_to_re,\n",
    "                    tokens = df['tokens'],\n",
    "                    tokens_clean = txt_tokens,\n",
    "                    input_ids=input_ids,\n",
    "                    offset_mapping_init = offset_mapping_init,\n",
    "                    re_offset_mapping = re_offset_mapping,\n",
    "                    attention_mask=attention_mask,\n",
    "                    word_boxes=word_boxes,\n",
    "                    gt_spans=gt_spans)\n",
    "    # ======================================================================================== #\n",
    "    def name_student(self,v):\n",
    "        # Add Name for Name/ and Mobile/Tel for phone Email for mail\n",
    "        text = random.choices([f\"Reflection â€“ Visualization {v}\",f'Person {v}',\n",
    "                       f\"STORYTELLER {v}\",f\"STORY TELLING {v}\",f\"{v}\"],k=1,weights = [0.125,0.125,0.125,0.125,0.5])[0]\n",
    "\n",
    "        return text\n",
    "    # ======================================================================================== #\n",
    "    def generate_fake_data(self):\n",
    "\n",
    "        data = self.generate_random_data_with_probabilities()\n",
    "\n",
    "        NB_PII_MAX = random.choice([1,2,3])\n",
    "        piis_ent = random.sample(list(data.keys()),k=NB_PII_MAX)\n",
    "\n",
    "\n",
    "        full_text = \"\"\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        offset_mapping = []\n",
    "        off = 0\n",
    "        for num, ent in enumerate(piis_ent):\n",
    "\n",
    "            if ent==\"NAME_STUDENT\":\n",
    "                text = self.name_student(data[ent])\n",
    "                if num ==0:\n",
    "                    full_text = text\n",
    "                else:\n",
    "                    off = off + 1 \n",
    "                    full_text = full_text + \" \" + text\n",
    "\n",
    "                toks = self.tokenize_with_spacy(text)\n",
    "                tokns = toks['tokens']\n",
    "                offset = toks['offset_mapping']\n",
    "                labs = np.array([\"O\"]*len(tokns),dtype='<U50')\n",
    "                idx = [i for i,x in enumerate(tokns) if x in data[ent]][0]\n",
    "                labs[idx:] = \"NAME_STUDENT\"\n",
    "\n",
    "                tokens = tokens + tokns\n",
    "                labels = labels + labs.tolist()\n",
    "                new_offset = [(x[0]+off,x[1]+off) for x in offset]\n",
    "                offset_mapping = offset_mapping + new_offset\n",
    "                off = offset_mapping[-1][1]\n",
    "\n",
    "            else:\n",
    "                text = data[ent]\n",
    "\n",
    "                if num ==0:\n",
    "                    full_text = text\n",
    "                else:\n",
    "                    off = off+1\n",
    "                    full_text = full_text + \" \" + text\n",
    "\n",
    "                toks = self.tokenize_with_spacy(text)\n",
    "                tokns = toks['tokens']\n",
    "                offset = toks['offset_mapping']\n",
    "                labs = [ent]*len(tokns)\n",
    "\n",
    "                tokens = tokens + tokns\n",
    "                labels = labels + labs\n",
    "\n",
    "                new_offset = [(x[0]+off,x[1]+off) for x in offset]\n",
    "                offset_mapping = offset_mapping + new_offset\n",
    "                off = offset_mapping[-1][1]\n",
    "\n",
    "        return full_text,tokens,labels,offset_mapping\n",
    "\n",
    "    def remove_double_spaces(self,text):\n",
    "        # Use a regular expression to replace consecutive spaces with a single space\n",
    "        # cleaned_text = re.sub(r'\\s{2,}', ' | ', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "        return cleaned_text\n",
    "\n",
    "    def clean_text(self,text):\n",
    "        text = text.replace(u'\\x9d', u' ')\n",
    "        # text = resolve_encodings_and_normalize(text)\n",
    "        # text = text.replace(u'\\xa0', u' ')\n",
    "        # text = text.replace(u'\\x85', u'\\n')\n",
    "        text = self.remove_double_spaces(text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    # ======================================================================================== #\n",
    "    def find_successive_numbers(self,input_array):\n",
    "        result = []\n",
    "        current_sublist = []\n",
    "\n",
    "        for num in input_array:\n",
    "            if not current_sublist or num == current_sublist[-1] + 1:\n",
    "                current_sublist.append(num)\n",
    "            else:\n",
    "                result.append(current_sublist)\n",
    "                current_sublist = [num]\n",
    "\n",
    "        if current_sublist:\n",
    "            result.append(current_sublist)\n",
    "\n",
    "        return result\n",
    "    # ======================================================================================== #\n",
    "\n",
    "    def generate_random_number(self,length):\n",
    "        return ''.join(random.choice('0123456789') for _ in range(length))\n",
    "    # ======================================================================================== #\n",
    "    def generate_fake_social_media_urls(self,num_urls=1):\n",
    "        social_media_platforms = {\n",
    "            'LinkedIn': 'linkedin.com/in/',\n",
    "            'YouTube': 'youtube.com/c/',\n",
    "            'Instagram': 'instagram.com/',\n",
    "            'GitHub': 'github.com/',\n",
    "            'Facebook': 'facebook.com/',\n",
    "            'Twitter': 'twitter.com/'\n",
    "        }\n",
    "\n",
    "        fake_social_media_urls = []\n",
    "\n",
    "        for _ in range(num_urls):\n",
    "            fake_user_name = fake.user_name()\n",
    "            platform, domain = random.choice(list(social_media_platforms.items()))\n",
    "            fake_url = f'https://{domain}{fake_user_name}'\n",
    "            fake_social_media_urls.append(fake_url)\n",
    "\n",
    "        return fake_social_media_urls[0]\n",
    "    # ======================================================================================== #\n",
    "    def generate_random_data_with_probabilities(self):\n",
    "\n",
    "        name = random.choices([fake.name(),fake.first_name(), fake.last_name()],\n",
    "                              weights = [0.7,0.15,0.15], k = 1)[0]  #generic.person.full_name()\n",
    "        phone_number =  fake.phone_number()\n",
    "        username = fake.user_name()\n",
    "        email = fake.ascii_free_email()\n",
    "        address = fake.address()\n",
    "        id_num = random.choices([fake.passport_number(),fake.bban(),\n",
    "                                 fake.iban(),self.generate_random_number(12)],k=1,weights = [0.1,0.10,0.15,0.65])[0]\n",
    "        url_pers = self.generate_fake_social_media_urls()\n",
    "\n",
    "        ret = dict(\n",
    "                  NAME_STUDENT=name,\n",
    "                  EMAIL=email,\n",
    "                  USERNAME=username,\n",
    "                  ID_NUM=id_num,\n",
    "                  URL_PERSONAL=url_pers,\n",
    "                  PHONE_NUM=phone_number,\n",
    "                  STREET_ADDRESS=address\n",
    "                  )\n",
    "\n",
    "        for k,v in ret.items():\n",
    "            ret[k] = self.clean_text(v.replace(\"\\n\\n\",\" | \").replace(\"\\n\",\" [BR] \"))\n",
    "        return ret\n",
    "    # ======================================================================================== #\n",
    "    def generate_ent(self,text,labels,offset_mapping):\n",
    "\n",
    "        idx_lab = np.argwhere(np.array(labels)!=\"O\").reshape(-1)\n",
    "        pos = self.find_successive_numbers(idx_lab)\n",
    "        lab = np.array(labels)\n",
    "\n",
    "\n",
    "        ent = {}\n",
    "        ent_order = []\n",
    "        ent_offset_in_order = []\n",
    "        for i,p in enumerate(pos):\n",
    "            l = [x.split('-')[-1] for x in lab[p]]\n",
    "\n",
    "            if len(np.unique(l))>1:\n",
    "                px = self.successive_positions(l)\n",
    "                for pp in px:\n",
    "                    full_name = text[offset_mapping[p[pp[0]]][0]:offset_mapping[p[pp[-1]]][1]].strip()\n",
    "                    ent[full_name] = l[pp[-1]]\n",
    "                    ent_order.append(full_name)\n",
    "                    ent_offset_in_order.append((offset_mapping[p[pp[0]]][0],\n",
    "                                                offset_mapping[p[pp[-1]]][1]))\n",
    "\n",
    "            else:\n",
    "                full_name = text[offset_mapping[p[0]][0]:offset_mapping[p[-1]][1]].strip()\n",
    "                ent[full_name] = l[-1]\n",
    "                ent_order.append(full_name)\n",
    "                ent_offset_in_order.append((offset_mapping[p[0]][0],offset_mapping[p[-1]][1]))\n",
    "\n",
    "        return ent,ent_order,ent_offset_in_order\n",
    "    # ======================================================================================== #\n",
    "    def tokenize_with_spacy(self,text,tok=EN_TOK):\n",
    "        tokenized_text = tok(text)\n",
    "        tokens = [token.text for token in tokenized_text]\n",
    "        offset_mapping = [(token.idx,token.idx+len(token)) for token in tokenized_text]\n",
    "        return {'tokens': tokens, 'offset_mapping': offset_mapping}\n",
    "    \n",
    "    def successive_positions(self,input_list):\n",
    "        result = []\n",
    "        current_group = []\n",
    "        prev_element = None\n",
    "\n",
    "        for i, element in enumerate(input_list):\n",
    "            if element == prev_element:\n",
    "                current_group.append(i)\n",
    "            else:\n",
    "                if current_group:\n",
    "                    result.append(current_group)\n",
    "                current_group = [i]\n",
    "            prev_element = element\n",
    "\n",
    "        if current_group:\n",
    "            result.append(current_group)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # ======================================================================================== #\n",
    "    def get_offset_mapping(self,full_text, tokens):\n",
    "        offset_mapping = []\n",
    "\n",
    "        current_offset = 0\n",
    "        for token in tokens:\n",
    "            start, end = self.get_text_start_end(full_text, token, search_from=current_offset)\n",
    "            offset_mapping.append((start, end))\n",
    "            current_offset = end\n",
    "\n",
    "        return offset_mapping\n",
    "    # ======================================================================================== #\n",
    "    def create_mapper_n_clean(self,full_text,labels,offset_mapping,attribut=[\"NAME_STUDENT\"]):\n",
    "        ent,ent_order,ent_offset_in_order = self.generate_ent(full_text,labels,offset_mapping)\n",
    "\n",
    "        mapper = {}\n",
    "        label_mapper = {}\n",
    "        new_tokens = []\n",
    "        txt_added = 0\n",
    "        for num,k in enumerate(ent_order):\n",
    "            v = ent[k]\n",
    "\n",
    "            if v in attribut:      \n",
    "                dc_ent = self.generate_random_data_with_probabilities()\n",
    "                mapper[k] = dc_ent[v]\n",
    "                label_mapper[dc_ent[v]] = v\n",
    "\n",
    "                old_len = len(full_text)\n",
    "                if k in mapper.keys():\n",
    "                    full_text = full_text[:ent_offset_in_order[num][0]+txt_added] +\" \" +mapper[k] + \" \"+full_text[ent_offset_in_order[num][-1]+txt_added:]\n",
    "                    txt_added+= len(full_text)-old_len\n",
    "\n",
    "                    new_tokens.append(mapper[k])\n",
    "                else:\n",
    "                    full_text = full_text[:ent_offset_in_order[num][0]+txt_added] + \" \"+dc_ent[v] +\" \"+ full_text[ent_offset_in_order[num][-1]+txt_added:]\n",
    "\n",
    "                    new_tokens.append(dc_ent[v])\n",
    "                    txt_added+= len(full_text)-old_len\n",
    "            else:\n",
    "                label_mapper[k] = v\n",
    "                new_tokens.append(k)\n",
    "\n",
    "        full_text = self.clean_text(full_text)\n",
    "\n",
    "        tokenized_text = self.tokenize_with_spacy(full_text)\n",
    "        tokens = tokenized_text['tokens']\n",
    "        tg = self.get_offset_mapping(full_text, new_tokens)\n",
    "\n",
    "\n",
    "        woff = np.array(tokenized_text['offset_mapping'])\n",
    "        labels = np.array([\"O\"]*len(woff),dtype='<U50')\n",
    "\n",
    "        toff = np.array(tg)\n",
    "        wx1, wx2 = woff.T\n",
    "        tx1, tx2 = toff.T\n",
    "        ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n",
    "        ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n",
    "        ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n",
    "        ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n",
    "        ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1)\n",
    "\n",
    "\n",
    "        for i,row in enumerate(ious):\n",
    "            inds = row.nonzero()[0]\n",
    "            if len(inds):\n",
    "                labels[i] = label_mapper[new_tokens[inds[0]]]\n",
    "\n",
    "        labels = labels.tolist()\n",
    "\n",
    "        return full_text,labels,tokens\n",
    "\n",
    "    # ======================================================================================== #\n",
    "    def get_text_start_end(self,txt, s, search_from=0):\n",
    "        txt = txt[int(search_from):]\n",
    "        try:\n",
    "            idx = txt.find(s)\n",
    "            if idx >= 0:\n",
    "                st = idx\n",
    "                ed = st + len(s)\n",
    "            else:\n",
    "                raise ValueError('Error')\n",
    "        except:\n",
    "            res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "            if len(res):\n",
    "                st, ed = res[0][0], res[0][1]\n",
    "            else:\n",
    "                m = SequenceMatcher(None, s, txt).get_opcodes()\n",
    "                for tag, i1, i2, j1, j2 in m:\n",
    "                    if tag == 'replace':\n",
    "                        s = s[:i1] + txt[j1:j2] + s[i2:]\n",
    "                    if tag == \"delete\":\n",
    "                        s = s[:i1] + s[i2:]\n",
    "\n",
    "                res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "                if len(res):\n",
    "                    st, ed = res[0][0], res[0][1]\n",
    "                else:\n",
    "                    idx = txt.find(s)\n",
    "                    if idx >= 0:\n",
    "                        st = idx\n",
    "                        ed = st + len(s)\n",
    "                    else:\n",
    "                        st, ed = 0, 0\n",
    "        return st + search_from, ed + search_from\n",
    "    \n",
    "    # ======================================================================================== #\n",
    "    def find_patterns(self,text,regex):\n",
    "        matches = [(match.group(0), match.start(), match.end()) for match in regex.finditer(text)]\n",
    "        offsets = self.strip_offset_mapping(text,[(m[1],m[2]) for m in matches])\n",
    "        return [m[0].strip() for m in matches],offsets\n",
    "    # ======================================================================================== #\n",
    "    def spacy_to_re_off(self,text,tokens,offset_mapping_init):\n",
    "        \n",
    "\n",
    "        lab,off_matc = self.find_patterns(text,REGEX_COMPILE)\n",
    "\n",
    "\n",
    "        if len(lab):\n",
    "\n",
    "            spacy_to_re = []\n",
    "            re_oof = []\n",
    "            tokens_re = []\n",
    "\n",
    "            # token slices of words\n",
    "            woff = np.array(offset_mapping_init)\n",
    "            toff = off_matc\n",
    "            wx1, wx2 = woff.T\n",
    "            tx1, tx2 = toff.T\n",
    "            ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n",
    "            ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n",
    "            ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n",
    "            ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n",
    "            ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1+1e-12)\n",
    "\n",
    "            for i,(spcy_of_set,tok,row) in enumerate(zip(offset_mapping_init,tokens,ious)):\n",
    "                inds = row.nonzero()[0]\n",
    "                if len(inds):\n",
    "                    spacy_to_re.append(inds[0])\n",
    "                    re_oof.append((off_matc[inds[0]].tolist()[0],off_matc[inds[0]].tolist()[1]))\n",
    "                    tokens_re.append(lab[inds[0]] if len(lab[inds[0]])>len(tok) else tok)\n",
    "                else:\n",
    "                    spacy_to_re.append(len(lab)+i)\n",
    "                    re_oof.append(spcy_of_set)\n",
    "                    tokens_re.append(tok)\n",
    "\n",
    "            re_oof = [(x,i) for i, x in enumerate(re_oof) if re_oof.index(x) == i]\n",
    "            spacy_to_re_unique = [spacy_to_re[x[1]] for x in re_oof]\n",
    "            re_oof = [re_oof[x[0]] for x in re_oof]\n",
    "        else:\n",
    "            spacy_to_re = np.arange(len(offset_mapping_init)).tolist()\n",
    "            re_oof = offset_mapping_init\n",
    "            tokens_re = tokens\n",
    "\n",
    "        return re_oof,spacy_to_re,tokens_re,spacy_to_re_unique\n",
    "    # ======================================================================================== #\n",
    "    def strip_offset_mapping(self, text, offset_mapping):\n",
    "        ret = []\n",
    "        for start, end in offset_mapping:\n",
    "            match = list(re.finditer('\\S+', text[start:end]))\n",
    "            if len(match) == 0:\n",
    "                ret.append((start, end))\n",
    "            else:\n",
    "                span_start, span_end = match[0].span()\n",
    "                ret.append((start + span_start, start + span_end))\n",
    "        return np.array(ret)\n",
    "    # ======================================================================================== #\n",
    "    def get_word_offsets(self, text):\n",
    "        matches = re.finditer(\"\\S+\", text)\n",
    "        spans = []\n",
    "        words = []\n",
    "        for match in matches:\n",
    "            span = match.span()\n",
    "            word = match.group()\n",
    "            spans.append(span)\n",
    "            words.append(word)\n",
    "        assert tuple(words) == tuple(text.split())\n",
    "        return np.array(spans)\n",
    "    # ======================================================================================== #\n",
    "    def custom_distribution(self,n):\n",
    "        distribution = [0] * n\n",
    "        middle_index = n // 2\n",
    "        for i in range(middle_index):\n",
    "            distribution[i] = (middle_index - i) / middle_index\n",
    "            distribution[n - 1 - i] = (middle_index - i) / middle_index\n",
    "        return distribution\n",
    "    # ======================================================================================== #\n",
    "    def add_text(self,full_text,tokens,labels,offset_mapping,\n",
    "                 new_text,new_tokens,new_labels,new_offset_mapping):\n",
    "        try:\n",
    "            s = full_text.split('|')\n",
    "            prob_dist = self.custom_distribution(len(s))\n",
    "            id_ = random.choices(np.arange(len(s)),k=1,weights = prob_dist)[0]\n",
    "\n",
    "            idx = [len(s[i]) for i in range(id_+1)]\n",
    "            idx = sum(idx)\n",
    "            full_text = full_text[:idx+id_+1] +\" \"+ new_text + full_text[idx+id_+1:]\n",
    "\n",
    "\n",
    "            t_idx = [i for i,x in enumerate(offset_mapping) if x[1]==idx+id_+1][-1]+1\n",
    "\n",
    "\n",
    "            tokens = tokens[:t_idx]+new_tokens+tokens[t_idx:]\n",
    "            labels = labels[:t_idx]+new_labels+labels[t_idx:]\n",
    "\n",
    "\n",
    "            v = offset_mapping[:t_idx][-1][1]\n",
    "            new_offset_mappings = [(x[0]+v+1,x[1]+v+1) for x in new_offset_mapping]\n",
    "            v1 = new_offset_mappings[-1][1]\n",
    "            vx = v1-v\n",
    "            \n",
    "            old_offset_mapping =  [(x[0]+vx,x[1]+vx) for x in offset_mapping[t_idx:]]\n",
    "            offset_mapping = offset_mapping[:t_idx]+new_offset_mappings+old_offset_mapping\n",
    "\n",
    "        except:\n",
    "            print(\"Text not added\")\n",
    "        return full_text,tokens,labels,offset_mapping\n",
    "\n",
    "## =============================================================================== ##\n",
    "##                                                                                 ##\n",
    "## =============================================================================== ##\n",
    "\n",
    "class CustomCollator(object):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        if hasattr(model.config, 'attention_window'):\n",
    "            # For longformer\n",
    "            # https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/longformer/modeling_longformer.py#L1548\n",
    "            self.attention_window = (model.config.attention_window\n",
    "                                     if isinstance(\n",
    "                                         model.config.attention_window, int)\n",
    "                                     else max(model.config.attention_window))\n",
    "        else:\n",
    "            self.attention_window = None\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        batch_size = len(samples)\n",
    "        assert batch_size == 1, f'Only batch_size=1 supported, got batch_size={batch_size}.'\n",
    "\n",
    "        sample = samples[0]\n",
    "\n",
    "        max_seq_length = len(sample['input_ids'])\n",
    "        if self.attention_window is not None:\n",
    "            attention_window = self.attention_window\n",
    "            padded_length = (attention_window -\n",
    "                             max_seq_length % attention_window\n",
    "                             ) % attention_window + max_seq_length\n",
    "        else:\n",
    "            padded_length = max_seq_length\n",
    "\n",
    "        input_shape = (1, padded_length)\n",
    "        input_ids = torch.full(input_shape,\n",
    "                               self.pad_token_id,\n",
    "                               dtype=torch.long)\n",
    "        attention_mask = torch.zeros(input_shape, dtype=torch.long)\n",
    "\n",
    "        seq_length = len(sample['input_ids'])\n",
    "        input_ids[0, :seq_length] = sample['input_ids']\n",
    "        attention_mask[0, :seq_length] = sample['attention_mask']\n",
    "\n",
    "        text_id = sample['text_id']\n",
    "        tokens = sample['tokens']\n",
    "        tokens_clean = sample['tokens_clean']\n",
    "        re_tokens = sample['re_tokens']\n",
    "        spacy_to_re = sample['spacy_to_re']\n",
    "        # text = sample['text']\n",
    "        word_boxes = sample['word_boxes']\n",
    "        gt_spans = sample['gt_spans']\n",
    "\n",
    "        return dict(text_id=text_id,\n",
    "                    tokens_clean=tokens_clean,\n",
    "                    tokens = tokens,\n",
    "                    re_tokens = re_tokens,\n",
    "                    spacy_to_re = spacy_to_re,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    word_boxes=word_boxes,\n",
    "                    gt_spans=gt_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f9ef245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2610f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.full_text.str.contains('\\?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d33c7f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6807 samples.\n"
     ]
    }
   ],
   "source": [
    "ds = TrainDataset(df.copy(),tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "105f6de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(461, 461, 461, 458)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.choice(ds.df[ds.df.PHONE_NUM>0].index)\n",
    "# idx = 428\n",
    "# doc = 204\n",
    "# idx = ds.df[ds.df.document==9854].index[0]\n",
    "# Example usage:\n",
    "# idx = 80\n",
    "d = ds[idx]\n",
    "full_text_ds = d['text']\n",
    "tokens_ds = d['tokens_clean']\n",
    "labels_ds = d['labels']\n",
    "offset_mapping_init = d['offset_mapping_init']\n",
    "re_offset_mapping = d['re_offset_mapping']\n",
    "spacy_to_re = d['spacy_to_re']\n",
    "re_tokens = d['re_tokens']\n",
    "# offset_mapping_ds = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "# offset_mapping_ds1 = tokenize_with_spacy(full_text_ds)['offset_mapping']\n",
    "# offset_mapping_ds = strip_offset_mapping(full_text_ds,offset_mapping_ds)\n",
    "# idx,ds.df.iloc[idx]['nb_labels']\n",
    "len(tokens_ds),len(offset_mapping_init),len(re_tokens),len(re_offset_mapping) #len(offset_mapping_ds1),len(tokens_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a27d1269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "39dce0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================== #\n",
    "def get_text_start_end(txt, s, search_from=0):\n",
    "    txt = txt[int(search_from):]\n",
    "    try:\n",
    "        idx = txt.find(s)\n",
    "        if idx >= 0:\n",
    "            st = idx\n",
    "            ed = st + len(s)\n",
    "        else:\n",
    "            raise ValueError('Error')\n",
    "    except:\n",
    "        res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "        if len(res):\n",
    "            st, ed = res[0][0], res[0][1]\n",
    "        else:\n",
    "            m = SequenceMatcher(None, s, txt).get_opcodes()\n",
    "            for tag, i1, i2, j1, j2 in m:\n",
    "                if tag == 'replace':\n",
    "                    s = s[:i1] + txt[j1:j2] + s[i2:]\n",
    "                if tag == \"delete\":\n",
    "                    s = s[:i1] + s[i2:]\n",
    "\n",
    "            res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "            if len(res):\n",
    "                st, ed = res[0][0], res[0][1]\n",
    "            else:\n",
    "                idx = txt.find(s)\n",
    "                if idx >= 0:\n",
    "                    st = idx\n",
    "                    ed = st + len(s)\n",
    "                else:\n",
    "                    st, ed = 0, 0\n",
    "    return st + search_from, ed + search_from\n",
    "    \n",
    "def get_offset_mapping(full_text, tokens):\n",
    "    offset_mapping = []\n",
    "\n",
    "    current_offset = 0\n",
    "    for token in tokens:\n",
    "        start, end = get_text_start_end(full_text, token, search_from=current_offset)\n",
    "        offset_mapping.append((start, end))\n",
    "        current_offset = end\n",
    "\n",
    "    return offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "662e869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.df.iloc[idx]['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0a36a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = df.iloc[idx]['full_text']\n",
    "tokens = df.iloc[idx]['tokens']\n",
    "labels = df.iloc[idx]['labels']\n",
    "offset_mapping = get_offset_mapping(full_text, tokens)\n",
    "# idx,df.iloc[idx]['nb_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7b6c3179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eslam: (2168, 2173) : B-NAME_STUDENT\n",
      "Abo: (2174, 2177) : I-NAME_STUDENT\n",
      "Fatma: (2178, 2183) : I-NAME_STUDENT\n",
      "murraythomas@gmail.com: (2208, 2230) : B-EMAIL\n",
      "(: (2237, 2238) : B-PHONE_NUM\n",
      "223)392: (2238, 2245) : I-PHONE_NUM\n",
      "-: (2245, 2246) : I-PHONE_NUM\n",
      "2765: (2246, 2250) : I-PHONE_NUM\n"
     ]
    }
   ],
   "source": [
    "offset_mapping = get_offset_mapping(full_text, tokens)\n",
    "for token, offset,l in zip(tokens, offset_mapping,labels):\n",
    "    if l!=\"O\":\n",
    "        print(f\"{token}: {offset} : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a185a4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([458, 4])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['word_boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0ef6bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment: Reflection on Design Thinking, Storytelling- December 2018 | 1. Challenge and Selection | Mitchell Payne burnettnatalie@gmail.com Unit 8123 Box 7933 [BR] DPO AP 17819 Storytelling is a true way to be used in letting companyâ€™s productivity to be observed towards its achievements. Within different methods, professional companies do elaborate some approaches where storytelling is part of strategies to annual action plans. As head of finance in Financial institution, after my personal observations to the staff performance, I realized that they do perform well under direct supervision than working on their own spirt. This has motivated me to choose storytelling as a best tool to change their mindset. | 2. Application | As a head of Finance, one day I organized a meeting with my staff in my department. When I was conducting my weekly monitoring, I found that, a professional meeting has to be chaired. I call upon my staffs from whom I am supervisor toward personal productivity. During that discussion, I told them a story from which everyone had to react on his/her point of view. | When we were in that presentations, the audience were attracted as time run out. After that reaction, without any kind of sampling, each from them has been allowed free time to express his/her concern. They arguments, easily, helped me to get their term modification towards their performance resulting from the story read down above. | 3. Insight | By following all courses planned, I learnt from in many things. Firstly, from design thinking, I served that, design thinking is a good tool to be used in problem solving. It helped me also to respect every oneâ€™s opinion within in a given institution toward its production. | 4. Approach | As I am allowed an opportunity to follow Design Thinking for Innovation as a course, it will help me to know some needs, choices, tastes and preferences of staff, family members, clients and others beneficiaries in general. After its implementation I do predict a positive change all listed categories of concerned above respectively. | I do conclude, my assignment by thanking Lecturers, University of Virginia and other partners who contributed to this online courses. | May God bless you. | Eslam Abo Fatma | Rwanda- Africa | Email: murraythomas@gmail.com | Tel: (223)392-2765 |\n"
     ]
    }
   ],
   "source": [
    "# text = \"Reflection â€“ Visualization   Deiby\"\n",
    "print((full_text_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d724048f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_re</th>\n",
       "      <th>labels</th>\n",
       "      <th>spacy_to_re</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assignment</td>\n",
       "      <td>Assignment</td>\n",
       "      <td>O</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reflection</td>\n",
       "      <td>Reflection</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8123</td>\n",
       "      <td>8123</td>\n",
       "      <td>STREET_ADDRESS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7933</td>\n",
       "      <td>7933</td>\n",
       "      <td>STREET_ADDRESS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17819</td>\n",
       "      <td>17819</td>\n",
       "      <td>STREET_ADDRESS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>(</td>\n",
       "      <td>(223)392-2765</td>\n",
       "      <td>B-PHONE_NUM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>223)392</td>\n",
       "      <td>(223)392-2765</td>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>-</td>\n",
       "      <td>(223)392-2765</td>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2765</td>\n",
       "      <td>(223)392-2765</td>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tokens      tokens_re          labels  spacy_to_re\n",
       "0    Assignment     Assignment               O            5\n",
       "1             :              :               O            6\n",
       "2    Reflection     Reflection               O            7\n",
       "3            on             on               O            8\n",
       "9          2018           2018               O            0\n",
       "21         8123           8123  STREET_ADDRESS            1\n",
       "23         7933           7933  STREET_ADDRESS            2\n",
       "29        17819          17819  STREET_ADDRESS            3\n",
       "456           (  (223)392-2765     B-PHONE_NUM            4\n",
       "457     223)392  (223)392-2765     I-PHONE_NUM            4\n",
       "458           -  (223)392-2765     I-PHONE_NUM            4\n",
       "459        2765  (223)392-2765     I-PHONE_NUM            4"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame({'tokens':tokens_ds,'tokens_re':re_tokens,\n",
    "              'labels':labels_ds,'spacy_to_re':spacy_to_re})\n",
    "\n",
    "x[x.spacy_to_re<=8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7212c7f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_re</th>\n",
       "      <th>labels</th>\n",
       "      <th>spacy_to_re</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>(</td>\n",
       "      <td>(223)392-2765</td>\n",
       "      <td>B-PHONE_NUM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>223)392</td>\n",
       "      <td>(223)392-2765</td>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>-</td>\n",
       "      <td>(223)392-2765</td>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2765</td>\n",
       "      <td>(223)392-2765</td>\n",
       "      <td>I-PHONE_NUM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokens      tokens_re       labels  spacy_to_re\n",
       "456        (  (223)392-2765  B-PHONE_NUM            4\n",
       "457  223)392  (223)392-2765  I-PHONE_NUM            4\n",
       "458        -  (223)392-2765  I-PHONE_NUM            4\n",
       "459     2765  (223)392-2765  I-PHONE_NUM            4"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[x.tokens!=x.tokens_re]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d2b10295",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_mapping = get_offset_mapping(full_text, tokens)\n",
    "offset_mapping_ = [x for (x,y) in zip(offset_mapping,labels) if y!=\"O\"]\n",
    "labels_ = [x for x in labels if x!=\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1f270391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Assignment: Reflection on Design Thinking, Storytelling- December 2018<br><br>1. Challenge and Selection<br><br>Storytelling is a true way to be used in letting companyâ€™s productivity to be observed  towards its achievements. Within different methods, professional companies do elaborate  some approaches where storytelling is part of strategies to annual action plans.  As head of finance in Financial institution, after my personal observations to the staff  performance, I realized that they do perform well under direct supervision than working on  their own spirt. This has motivated me to choose storytelling as a best tool to change their  mindset.<br><br>2. Application<br><br>As a head of Finance, one day I organized a meeting with my staff in my department. When  I was conducting my weekly monitoring, I found that, a professional meeting has to be  chaired. I call upon my staffs from whom I am supervisor toward personal productivity.  During that discussion, I told them a story from which everyone had to react on his/her  point of view.<br><br>When we were in that presentations, the audience were attracted as time run out. After that  reaction, without any kind of sampling, each from them has been allowed free time to  express his/her concern. They arguments, easily, helped me to get their term modification  towards their performance resulting from the story read down above.<br><br>3. Insight<br><br>By following all courses planned, I learnt from in many things. Firstly, from design  thinking, I served that, design thinking is a good tool to be used in problem solving. It  helped me also to respect every oneâ€™s opinion within in a given institution toward its  production.<br><br>4. Approach<br><br>As I am allowed an opportunity to follow Design Thinking for Innovation as a course, it  will help me to know some needs, choices, tastes and preferences of staff, family members,  clients and others beneficiaries in general. After its implementation I do predict a positive  change all listed categories of concerned above respectively.<br><br>I do conclude, my assignment by thanking Lecturers, University of Virginia and other  partners who contributed to this online courses.<br><br>May God bless you.<br><br>\n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Eslam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Abo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fatma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       "<br><br>Rwanda- Africa<br><br>Email: \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    murraythomas@gmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       "<br><br>Tel: \n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    223)392\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2765\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "<br><br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(full_text,offset_mapping_,labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7dc54c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset_mapping = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "offset_mapping_ = [x for (x,y) in zip(offset_mapping_init,labels_ds) if y!=\"O\"]\n",
    "labels_ = [x for x in labels_ds if x!=\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9f05249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (x,y) in zip(offset,labels_ds):\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a7ec06cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Assignment: Reflection on Design Thinking, Storytelling- December 2018 | 1. Challenge and Selection | \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mitchell\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Payne\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    burnettnatalie@gmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Unit\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    8123\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Box\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    7933\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    DPO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    17819\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " Storytelling is a true way to be used in letting companyâ€™s productivity to be observed towards its achievements. Within different methods, professional companies do elaborate some approaches where storytelling is part of strategies to annual action plans. As head of finance in Financial institution, after my personal observations to the staff performance, I realized that they do perform well under direct supervision than working on their own spirt. This has motivated me to choose storytelling as a best tool to change their mindset. | 2. Application | As a head of Finance, one day I organized a meeting with my staff in my department. When I was conducting my weekly monitoring, I found that, a professional meeting has to be chaired. I call upon my staffs from whom I am supervisor toward personal productivity. During that discussion, I told them a story from which everyone had to react on his/her point of view. | When we were in that presentations, the audience were attracted as time run out. After that reaction, without any kind of sampling, each from them has been allowed free time to express his/her concern. They arguments, easily, helped me to get their term modification towards their performance resulting from the story read down above. | 3. Insight | By following all courses planned, I learnt from in many things. Firstly, from design thinking, I served that, design thinking is a good tool to be used in problem solving. It helped me also to respect every oneâ€™s opinion within in a given institution toward its production. | 4. Approach | As I am allowed an opportunity to follow Design Thinking for Innovation as a course, it will help me to know some needs, choices, tastes and preferences of staff, family members, clients and others beneficiaries in general. After its implementation I do predict a positive change all listed categories of concerned above respectively. | I do conclude, my assignment by thanking Lecturers, University of Virginia and other partners who contributed to this online courses. | May God bless you. | \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Eslam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Abo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Fatma\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " | Rwanda- Africa | Email: \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    murraythomas@gmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       " | Tel: \n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    223)392\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2765\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       " |</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(full_text_ds,offset_mapping_,labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5fe4bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "en_tokenizer = English().tokenizer\n",
    "\n",
    "def tokenize_with_spacy(text, tokenizer=en_tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = [token.text for token in tokenized_text]\n",
    "    offset_mapping = [(token.idx,token.idx+len(token)) for token in tokenized_text]\n",
    "    return {'tokens': tokens, 'offset_mapping': offset_mapping}\n",
    "\n",
    "\n",
    "def remove_double_spaces(text):\n",
    "    # Use a regular expression to replace consecutive spaces with a single space\n",
    "    # cleaned_text = re.sub(r'\\s{2,}', ' | ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(u'\\x9d', u' ')\n",
    "    # text = resolve_encodings_and_normalize(text)\n",
    "    # text = text.replace(u'\\xa0', u' ')\n",
    "    # text = text.replace(u'\\x85', u'\\n')\n",
    "    text = remove_double_spaces(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# ======================================================================================== #\n",
    "def find_successive_numbers(input_array):\n",
    "    result = []\n",
    "    current_sublist = []\n",
    "\n",
    "    for num in input_array:\n",
    "        if not current_sublist or num == current_sublist[-1] + 1:\n",
    "            current_sublist.append(num)\n",
    "        else:\n",
    "            result.append(current_sublist)\n",
    "            current_sublist = [num]\n",
    "\n",
    "    if current_sublist:\n",
    "        result.append(current_sublist)\n",
    "\n",
    "    return result\n",
    "# ======================================================================================== #\n",
    "\n",
    "def generate_random_number(length):\n",
    "    return ''.join(random.choice('0123456789') for _ in range(length))\n",
    "# ======================================================================================== #\n",
    "def generate_fake_social_media_urls(num_urls=1):\n",
    "    social_media_platforms = {\n",
    "        'LinkedIn': 'linkedin.com/in/',\n",
    "        'YouTube': 'youtube.com/c/',\n",
    "        'Instagram': 'instagram.com/',\n",
    "        'GitHub': 'github.com/',\n",
    "        'Facebook': 'facebook.com/',\n",
    "        'Twitter': 'twitter.com/'\n",
    "    }\n",
    "\n",
    "    fake_social_media_urls = []\n",
    "\n",
    "    for _ in range(num_urls):\n",
    "        fake_user_name = fake.user_name()\n",
    "        platform, domain = random.choice(list(social_media_platforms.items()))\n",
    "        fake_url = f'https://{domain}{fake_user_name}'\n",
    "        fake_social_media_urls.append(fake_url)\n",
    "\n",
    "    return fake_social_media_urls[0]\n",
    "    \n",
    "# ======================================================================================== #\n",
    "def generate_random_data_with_probabilities():\n",
    "\n",
    "    name = random.choices([fake.name(),fake.first_name(), fake.last_name()],\n",
    "                          weights = [0.7,0.15,0.15], k = 1)[0]  #generic.person.full_name()\n",
    "    phone_number =  fake.phone_number()\n",
    "    username = fake.user_name()\n",
    "    email = fake.ascii_free_email()\n",
    "    address = fake.address()\n",
    "    id_num = random.choices([fake.passport_number(),fake.bban(),\n",
    "                             fake.iban(),generate_random_number(12)],k=1,weights = [0.1,0.10,0.15,0.65])[0]\n",
    "    url_pers = generate_fake_social_media_urls()\n",
    "\n",
    "    ret = dict(\n",
    "              NAME_STUDENT=name,\n",
    "              EMAIL=email,\n",
    "              USERNAME=username,\n",
    "              ID_NUM=id_num,\n",
    "              URL_PERSONAL=url_pers,\n",
    "              PHONE_NUM=phone_number,\n",
    "              STREET_ADDRESS=address\n",
    "              )\n",
    "\n",
    "    for k,v in ret.items():\n",
    "        ret[k] = clean_text(v.replace(\"\\n\\n\",\" | \").replace(\"\\n\",\" [BR] \"))\n",
    "    return ret\n",
    "# ======================================================================================== #\n",
    "def name_student(v):\n",
    "    # Add Name for Name/ and Mobile/Tel for phone Email for mail\n",
    "    text = random.choices([f\"Reflection â€“ Visualization {v}\",f'Person {v}',\n",
    "                   f\"STORYTELLER {v}\",f\"STORY TELLING {v}\",f\"{v}\"],k=1,weights = [0.125,0.125,0.125,0.125,0.5])[0]\n",
    "    \n",
    "    return text\n",
    "# ======================================================================================== #\n",
    "def generate_fake_data():\n",
    "    \n",
    "    data = generate_random_data_with_probabilities()\n",
    "    \n",
    "    NB_PII_MAX = random.choice([1,2,3])\n",
    "    piis_ent = random.sample(list(data.keys()),k=NB_PII_MAX)\n",
    "    \n",
    "    print(piis_ent)\n",
    "    \n",
    "    full_text = \"\"\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    offset_mapping = []\n",
    "    off = 0\n",
    "    for num, ent in enumerate(piis_ent):\n",
    "#         print(ent)\n",
    "        if ent==\"NAME_STUDENT\":\n",
    "            text = name_student(data[ent])\n",
    "            if num ==0:\n",
    "                full_text = text\n",
    "            else:\n",
    "                off = off + 1 \n",
    "                full_text = full_text + \" \" + text\n",
    "                \n",
    "            toks = tokenize_with_spacy(text)\n",
    "            tokns = toks['tokens']\n",
    "            offset = toks['offset_mapping']\n",
    "            labs = np.array([\"O\"]*len(tokns),dtype='<U50')\n",
    "            idx = [i for i,x in enumerate(tokns) if x in data[ent]][0]\n",
    "            labs[idx:] = \"NAME_STUDENT\"\n",
    "            \n",
    "            tokens = tokens + tokns\n",
    "            labels = labels + labs.tolist()\n",
    "            new_offset = [(x[0]+off,x[1]+off) for x in offset]\n",
    "            offset_mapping = offset_mapping + new_offset\n",
    "            off = offset_mapping[-1][1]\n",
    "#             print(off)\n",
    "        else:\n",
    "            text = data[ent]\n",
    "            \n",
    "            if num ==0:\n",
    "                full_text = text\n",
    "            else:\n",
    "                off = off+1\n",
    "                full_text = full_text + \" \" + text\n",
    "            \n",
    "            toks = tokenize_with_spacy(text)\n",
    "            tokns = toks['tokens']\n",
    "            offset = toks['offset_mapping']\n",
    "            labs = [ent]*len(tokns)\n",
    "            \n",
    "            tokens = tokens + tokns\n",
    "            labels = labels + labs\n",
    "            \n",
    "            new_offset = [(x[0]+off,x[1]+off) for x in offset]\n",
    "            offset_mapping = offset_mapping + new_offset\n",
    "            off = offset_mapping[-1][1]\n",
    "#             print(off)\n",
    "#         print(labels)\n",
    "    return full_text,tokens,labels,offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9e7b6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_random_data_with_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f3873b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STORY TELLING Ricky Smith'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = name_student(data['NAME_STUDENT'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "510e679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['STORY', 'TELLING', 'Ricky', 'Smith'],\n",
       " 'offset_mapping': [(0, 5), (6, 13), (14, 19), (20, 25)]}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_spacy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "18c411af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PHONE_NUM']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_PII_MAX = random.choice([1,2,3])\n",
    "random.sample(list(data.keys()),k=NB_PII_MAX,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "31b4e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PHONE_NUM', 'NAME_STUDENT', 'URL_PERSONAL']\n",
      "['PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM']\n",
      "['PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'O', 'O', 'O', 'NAME_STUDENT', 'NAME_STUDENT']\n",
      "['PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'O', 'O', 'O', 'NAME_STUDENT', 'NAME_STUDENT', 'URL_PERSONAL']\n"
     ]
    }
   ],
   "source": [
    "full_text,tokens,labels,offset_mapping = generate_fake_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "06b3b96a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    +1\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    943\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    777\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2994\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       " Reflection â€“ Visualization \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rowland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff8042; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    https://linkedin.com/in/watsonmatthew\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">URL_PERSONAL</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# offset_mapping = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "offset_mapping_ = [x for (x,y) in zip(offset_mapping,labels) if y!=\"O\"]\n",
    "labels_ = [x for x in labels if x!=\"O\"]\n",
    "visualize(full_text,offset_mapping_,labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "869a0268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+1-220-366-5660x479 143381494454 hernandezchristina'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "733fc99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8), (0, 12), (0, 34)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "40aff4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'N',\n",
       " 'A',\n",
       " 'M',\n",
       " 'E',\n",
       " '_',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'D',\n",
       " 'E',\n",
       " 'N',\n",
       " 'T',\n",
       " 'URL_PERSONAL']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
