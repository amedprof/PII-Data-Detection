{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d487dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Script/NLP/PII/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84bf958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1602e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.data_utils import get_offset_mapping,clean_text\n",
    "from data.dataset import FeedbackDataset,CustomCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7384a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650149f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from text_unidecode import unidecode\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# ======================================================================================== #\n",
    "def get_text_start_end(txt, s, search_from=0):\n",
    "    txt = txt[int(search_from):]\n",
    "    try:\n",
    "        idx = txt.find(s)\n",
    "        if idx >= 0:\n",
    "            st = idx\n",
    "            ed = st + len(s)\n",
    "        else:\n",
    "            raise ValueError('Error')\n",
    "    except:\n",
    "        res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "        if len(res):\n",
    "            st, ed = res[0][0], res[0][1]\n",
    "        else:\n",
    "            m = SequenceMatcher(None, s, txt).get_opcodes()\n",
    "            for tag, i1, i2, j1, j2 in m:\n",
    "                if tag == 'replace':\n",
    "                    s = s[:i1] + txt[j1:j2] + s[i2:]\n",
    "                if tag == \"delete\":\n",
    "                    s = s[:i1] + s[i2:]\n",
    "\n",
    "            res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "            if len(res):\n",
    "                st, ed = res[0][0], res[0][1]\n",
    "            else:\n",
    "                idx = txt.find(s)\n",
    "                if idx >= 0:\n",
    "                    st = idx\n",
    "                    ed = st + len(s)\n",
    "                else:\n",
    "                    st, ed = 0, 0\n",
    "    return st + search_from, ed + search_from\n",
    "    \n",
    "def get_offset_mapping(full_text, tokens):\n",
    "    offset_mapping = []\n",
    "\n",
    "    current_offset = 0\n",
    "    for token in tokens:\n",
    "        start, end = get_text_start_end(full_text, token, search_from=current_offset)\n",
    "        offset_mapping.append((start, end))\n",
    "        current_offset = end\n",
    "\n",
    "    return offset_mapping\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from pylab import cm, matplotlib\n",
    "import os\n",
    "\n",
    "colors = {\n",
    "            'NAME_STUDENT': '#8000ff',\n",
    "            'EMAIL': '#2b7ff6',\n",
    "            'USERNAME': '#2adddd',\n",
    "            'ID_NUM': '#80ffb4',\n",
    "            'PHONE_NUM': 'd4dd80',\n",
    "            'URL_PERSONAL': '#ff8042',\n",
    "            'STREET_ADDRESS': '#ff0000'\n",
    "         }\n",
    "\n",
    "\n",
    "def visualize(full_text,offset_mapping,labels):\n",
    "    \n",
    "    ents = []\n",
    "    for offset,lab in zip(offset_mapping,labels):\n",
    "        ents.append({\n",
    "                        'start': int(offset[0]), \n",
    "                         'end': int(offset[1]), \n",
    "                         'label': str(lab.split('-')[1]) #+ ' - ' + str(row['discourse_effectiveness'])\n",
    "                    })\n",
    "\n",
    "    doc2 = {\n",
    "        \"text\": full_text,\n",
    "        \"ents\": ents,\n",
    "#         \"title\": \"idx\"\n",
    "    }\n",
    "\n",
    "    options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48cd78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pylab import cm, matplotlib\n",
    "import os\n",
    "\n",
    "colors = {\n",
    "            'NAME_STUDENT': '#8000ff',\n",
    "            'EMAIL': '#2b7ff6',\n",
    "            'USERNAME': '#2adddd',\n",
    "            'ID_NUM': '#80ffb4',\n",
    "            'PHONE_NUM': 'd4dd80',\n",
    "            'URL_PERSONAL': '#ff8042',\n",
    "            'STREET_ADDRESS': '#ff0000'\n",
    "         }\n",
    "\n",
    "\n",
    "def visualize(full_text,offset_mapping,labels):\n",
    "    \n",
    "    ents = []\n",
    "    for offset,lab in zip(offset_mapping,labels):\n",
    "        ents.append({\n",
    "                        'start': int(offset[0]), \n",
    "                         'end': int(offset[1]), \n",
    "                         'label': str(lab.split('-')[-1]) #+ ' - ' + str(row['discourse_effectiveness'])\n",
    "                    })\n",
    "\n",
    "    doc2 = {\n",
    "        \"text\": full_text,\n",
    "        \"ents\": ents,\n",
    "#         \"title\": \"idx\"\n",
    "    }\n",
    "\n",
    "    options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895aef6",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0104cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.json',\n",
       " 'mpware_mixtral8x7b_v1.1-no-i-username.json',\n",
       " 'pii_dataset_fixed.csv',\n",
       " 'mixtral-8x7b-v1.json',\n",
       " '.~lock.lecture2.pptx#',\n",
       " 'Fake_data_1850_218.json',\n",
       " 'test.json',\n",
       " 'archive.zip',\n",
       " 'archive',\n",
       " 'pii-masking-200k.csv',\n",
       " 'sample_submission.csv',\n",
       " 'mpware_mixtral8x7b_v1.1.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(r\"/database/kaggle/PII/data\")\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc125ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6807, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(data_path/'train.json')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777aa2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design Thinking for innovation reflexion-Avril...</td>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[True, False, False, True, True, False, False,...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document                                          full_text                                             tokens                                trailing_whitespace                                             labels\n",
       "0         7  Design Thinking for innovation reflexion-Avril...  [Design, Thinking, for, innovation, reflexion,...  [True, True, True, True, False, False, True, F...  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...\n",
       "1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...  [True, False, False, True, True, False, False,...  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc7b295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL2TYPE = ('NAME_STUDENT','EMAIL','USERNAME','ID_NUM', 'PHONE_NUM','URL_PERSONAL','STREET_ADDRESS','O')\n",
    "len(LABEL2TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "400ffd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in LABEL2TYPE[:-1]:\n",
    "    df[name] = ((df['labels'].transform(lambda x:len([i for i in x if i.split('-')[-1]==name ])>0)))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6aa3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nb_labels'] = df['labels'].transform(lambda x:len([i for i in x if i!=\"O\" ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e76e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5862\n",
       "2      599\n",
       "4      108\n",
       "1       86\n",
       "3       52\n",
       "6       46\n",
       "8       14\n",
       "5       10\n",
       "12       6\n",
       "10       5\n",
       "11       3\n",
       "9        3\n",
       "15       2\n",
       "14       2\n",
       "21       2\n",
       "7        1\n",
       "23       1\n",
       "18       1\n",
       "17       1\n",
       "26       1\n",
       "34       1\n",
       "22       1\n",
       "Name: nb_labels, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nb_labels'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a59c9509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAME_STUDENT     0.8642\n",
       "EMAIL            0.0233\n",
       "USERNAME         0.0048\n",
       "ID_NUM           0.0320\n",
       "PHONE_NUM        0.0039\n",
       "URL_PERSONAL     0.0698\n",
       "STREET_ADDRESS   0.0019\n",
       "nb_labels        2.6566\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[list(LABEL2TYPE)[:-1]+['nb_labels']].sum()/1031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915cb0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "891+24+5+33+4+72+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b03e4c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "c264927d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.285714285714286"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db5e835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "417a4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42]\n",
    "folds_names = []\n",
    "for K in [5]:  \n",
    "    for seed in seeds:\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=K,shuffle=True,random_state=seed)\n",
    "        name = f\"fold_msk_{K}_seed_{seed}\"\n",
    "        df[name] = -1\n",
    "        for fold, (trn_, val_) in enumerate(mskf.split(df,df[list(LABEL2TYPE)[:-1]])):\n",
    "            df.loc[val_, name] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a92d9cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_STUDENT</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>USERNAME</th>\n",
       "      <th>ID_NUM</th>\n",
       "      <th>PHONE_NUM</th>\n",
       "      <th>URL_PERSONAL</th>\n",
       "      <th>STREET_ADDRESS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold_msk_5_seed_42</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    NAME_STUDENT  EMAIL  USERNAME  ID_NUM  PHONE_NUM  URL_PERSONAL  STREET_ADDRESS\n",
       "fold_msk_5_seed_42                                                                                \n",
       "0                            178      5         1       7          1            14               0\n",
       "1                            178      5         1       7          1            14               0\n",
       "2                            179      5         1       6          1            15               1\n",
       "3                            178      4         1       6          0            15               1\n",
       "4                            178      5         1       7          1            14               0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(name)[list(LABEL2TYPE)[:-1]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9ef245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58de9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from model_zoo.models import FeedbackModel,span_nms,aggregate_tokens_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95a377bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FeedbackModel(model_name,pooling_params={\"pooling_name\":\"MeanPooling\",'params':{}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2033db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.full_text.str.contains('\\?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb6bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl = CustomCollator(tokenizer,model)\n",
    "# dl = DataLoader(ds,collate_fn=cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ff924e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for data in tqdm(dl):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3ae27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1354b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(df):\n",
    "    \n",
    "    dmask = df[df.nb_labels>0].reset_index(drop=True)\n",
    "    ds = FeedbackDataset(dmask,\n",
    "                     tokenizer,use_re  = False,\n",
    "                     add_text_prob=1.,\n",
    "                     replace_text_prob=0.0,\n",
    "                     attrib_to_replace = ['NAME_STUDENT','EMAIL','USERNAME','ID_NUM',\n",
    "                                          'PHONE_NUM','URL_PERSONAL','STREET_ADDRESS'],\n",
    "                     inference=False)\n",
    "    \n",
    "    idx = random.choice(ds.df.index)\n",
    "    dx = ds[idx]\n",
    "    \n",
    "    full_text_ds = dx['text']\n",
    "    tokens_ds = dx['tokens_clean']\n",
    "    labels_ds = dx['labels']\n",
    "    offset_map_ds = dx['offset_mapping_init']\n",
    "\n",
    "    \n",
    "    full_text = dmask.iloc[idx]['full_text']\n",
    "    tokens = dmask.iloc[idx]['tokens']\n",
    "    labels = dmask.iloc[idx]['labels']\n",
    "    \n",
    "    \n",
    "    \n",
    "    offset_mapping = get_offset_mapping(full_text, tokens)\n",
    "    offset_mapping_ = [x for (x,y) in zip(offset_mapping,labels) if y!=\"O\"]\n",
    "    labels_ = [x for x in labels if x!=\"O\"]\n",
    "    \n",
    "    print(\" -------------------------------------- GT ---------------------------------------\")\n",
    "    print(\" --------------------------------------   ---------------------------------------\")\n",
    "    visualize(full_text,offset_mapping_,labels_)\n",
    "    \n",
    "    print(\" --------------------------------------   ---------------------------------------\\n\")\n",
    "    \n",
    "    print(\" -------------------------------------- Edit ---------------------------------------\")\n",
    "    print(\" --------------------------------------   ---------------------------------------\\n\")\n",
    "#     offset_mapping = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "    offset_mapping_ = [x for (x,y) in zip(offset_map_ds,labels_ds) if y!=\"O\"]\n",
    "    labels_ = [x for x in labels_ds if x!=\"O\"]\n",
    "    visualize(full_text_ds,offset_mapping_,labels_)\n",
    "    \n",
    "    \n",
    "#     print(\" -------------------------------------- Pred m2---------------------------------------\")\n",
    "#     print(\" --------------------------------------   ---------------------------------------\\n\")\n",
    "#     offset_mapping = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "#     offset_mapping_ = [x for (x,y,s) in zip(offset_mapping,labels2,scores2) if y!=\"O\" and s>0.15]\n",
    "#     labels_ = [x for (x,s) in zip(labels2,scores2) if x!=\"O\" and s>0.15]\n",
    "#     visualize(full_text_ds,offset_mapping_,labels_)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(\" -------------------------------------- Pred blend---------------------------------------\")\n",
    "#     print(\" --------------------------------------   ---------------------------------------\\n\")\n",
    "#     offset_mapping = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "#     offset_mapping_ = [x for (x,y,s) in zip(offset_mapping,labelsbl,scoresbl) if y!=\"O\" and s>0.15]\n",
    "#     labels_ = [x for (x,s) in zip(labelsbl,scoresbl) if x!=\"O\" and s>0.15]\n",
    "#     visualize(full_text_ds,offset_mapping_,labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a52998ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 945 samples.\n",
      "using add_text_prob 1.0\n",
      " -------------------------------------- GT ---------------------------------------\n",
      " --------------------------------------   ---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Reflection – Visualization<br><br>Course: Design Thinking for Innovation<br><br>Student: \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Randy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sanz\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       "<br><br>I.  Challenge<br><br>I was part of the A.T. Kearney account team for a Consumer Packaged Goods (CPG) customer in Lima, Perú. The  customer needed to implement a process to accelerate the time to market for new products. In order to do so  they needed to include all the steps required, from idea generation, through launch.<br><br>The customer had already selected the software they wanted to use which handled the core Product Lifecycle  Management (PLM) and engineering functions.  They wanted us to implement it for them.<br><br>As A.T. Kearney, we wanted to provide the customer additional value by proposing a more complete solution and  while doing so, include additional A.T. Kearney products. We needed to include these products in order get internal  approval since we were not allowed to sell and implement a non-A.T. Kearney product in isolation.<br><br>Our challenge was to design a solution that brought value to the customer by integrating a specific, third party  software with A.T. Kearney products. This software had never been integrated with the A.T. Kearney platform before.<br><br>II.  Selection<br><br>In Module 1 we reviewed a video by Angela Myer. In her video she tells us that visualization is applicable in  situations where we need to apply the right side of our brain, when we’re dealing with a creative process and  when we’re trying to figure out how to do something that has never done before.  We also learned that  visualization is an excellent tool for team collaboration.<br><br>In retrospect it made sense to apply visualization at this stage of our proposal development. We needed to gather  a diverse group: consulting, marketing, sales, technical specialists, and the software vendor (Afterscool) to build the  solution and we were trying to design something that had never been done before.  However, the reality was that  the choice was made as a matter of course, it seemed clear to all that we needed to whiteboard some ideas in  order to figure out the solution.<br><br>III.  Application<br><br>As the A.T. Kearney Consulting Services consultant on the team, it was my responsibility to lead the design. I basically  began with what we knew: the customer had already selected a specific product that needed to be included in the  solution. Based on this I drew a box in the middle of a blank whiteboard and we began to discuss different  possibilities. We started by trying to get an understanding of what the customer wanted to do with the software,  how this fit within their overall processes related with product launches and who all the corresponding  stakeholders were within the company.<br><br>After many meetings lasting several hours, all of which began with either a version of the solution or a completely  blank whiteboard, we came up with a model that visually described the components of the solution and their  interactions. This is a simplified version of the solution we designed:<br><br>IV.  Insight<br><br>An interesting insight from having done this exercise was that in addition to our stated goal of coming up with a  design, we learned a lot about the customer’s processes, its stakeholders, their interactions and ultimately about  the overall problem they wanted to solve. As a matter of fact, when we finally presented the solution to the  customer we realized that we not only added value by bringing a solution to the original customer problem but  that we helped the customer have an expanded view of the problem itself and the value they could gain.  The  customer did in the end decide to go ahead with the project which took 18 months to implement. Through these  18 months, the project followed almost completely, the design that we had come up with in those original  visualization meetings.<br><br>Having now gone through this course, this experience is a very personal proof point of how the design thinking  tools and processes can be applied in real world scenarios.  It also helps be feel more confident that with  additional research and practice, I can personally apply a design thinking approach in the future.<br><br>V.  Approach<br><br>The project was in the end successful by most measures; however, I believe it would have been very valuable to  conduct a mind mapping exercise with the customer, prior to designing the solution. This exercise would have  allowed us to do an even better job at understanding what the customer wanted to accomplish, what core  business value they could’ve attained.  With this improved understanding we might have been able to gain  additional insights and in turn proposed a better solution. We could have even been able to expand our  involvement to include additional critical areas of the business. By collaborating with the customer from the  beginning we would’ve deepened our relationship and improved our positioning as a strategic vendor.<br><br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --------------------------------------   ---------------------------------------\n",
      "\n",
      " -------------------------------------- Edit ---------------------------------------\n",
      " --------------------------------------   ---------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Reflection – Visualization | Course: Design Thinking for Innovation | Student: \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Randy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sanz\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " | I. Challenge | I was part of the A.T. Kearney account team for a Consumer Packaged Goods (CPG) customer in Lima, Perú. The customer needed to implement a process to accelerate the time to market for new products. In order to do so they needed to include all the steps required, from idea generation, through launch. | The customer had already selected the software they wanted to use which handled the core Product Lifecycle Management (PLM) and engineering functions. They wanted us to implement it for them. | As A.T. Kearney, we wanted to provide the customer additional value by proposing a more complete solution and while doing so, include additional A.T. Kearney products. We needed to include these products in order get internal approval since we were not allowed to sell and implement a non-A.T. Kearney product in isolation. | Our challenge was to design a solution that brought value to the customer by integrating a specific, third party software with A.T. Kearney products. This software had never been integrated with the A.T. Kearney platform before. | II. Selection | In Module 1 we reviewed a video by Angela Myer. In her video she tells us that visualization is applicable in situations where we need to apply the right side of our brain, when we’re dealing with a creative process and when we’re trying to figure out how to do something that has never done before. We also learned that visualization is an excellent tool for team collaboration. | In retrospect it made sense to apply visualization at this stage of our proposal development. We needed to gather a diverse group: consulting, marketing, sales, technical specialists, and the software vendor (Afterscool) to build the solution and we were trying to design something that had never been done before. However, the reality was that the choice was made as a matter of course, it seemed clear to all that we needed to whiteboard some ideas in order to figure out the solution. | III. Application | As the A.T. Kearney Consulting Services consultant on the team, it was my responsibility to lead the design. I basically began with what we knew: the customer had already selected a specific product that needed to be included in the solution. Based on this I drew a box in the middle of a blank whiteboard and we began to discuss different possibilities. We started by trying to get an understanding of what the customer wanted to do with the software, how this fit within their overall processes related with product launches and who all the corresponding stakeholders were within the company. | After many meetings lasting several hours, all of which began with either a version of the solution or a completely blank whiteboard, we came up with a model that visually described the components of the solution and their interactions. This is a simplified version of the solution we designed: | IV. Insight | An interesting insight from having done this exercise was that in addition to our stated goal of coming up with a design, we learned a lot about the customer’s processes, its stakeholders, their interactions and ultimately about the overall problem they wanted to solve. As a matter of fact, when we finally presented the solution to the customer we realized that we not only added value by bringing a solution to the original customer problem but that we helped the customer have an expanded view of the problem itself and the value they could gain. The customer did in the end decide to go ahead with the project which took 18 months to implement. Through these 18 months, the project followed almost completely, the design that we had come up with in those original visualization meetings. | Name \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Studio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    87\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    French\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mall\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Patelland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BT1V\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    9WP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " Having now gone through this course, this experience is a very personal proof point of how the design thinking tools and processes can be applied in real world scenarios. It also helps be feel more confident that with additional research and practice, I can personally apply a design thinking approach in the future. | V. Approach | The project was in the end successful by most measures; however, I believe it would have been very valuable to conduct a mind mapping exercise with the customer, prior to designing the solution. This exercise would have allowed us to do an even better job at understanding what the customer wanted to accomplish, what core business value they could’ve attained. With this improved understanding we might have been able to gain additional insights and in turn proposed a better solution. We could have even been able to expand our involvement to include additional critical areas of the business. By collaborating with the customer from the beginning we would’ve deepened our relationship and improved our positioning as a strategic vendor. |</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "105f6de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695, 695, 695, 695)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.choice(ds.df[ds.df.PHONE_NUM>0].index)\n",
    "# idx = 428\n",
    "# doc = 204\n",
    "# idx = ds.df[ds.df.document==9854].index[0]\n",
    "# Example usage:\n",
    "# idx = 80\n",
    "d = ds[idx]\n",
    "full_text_ds = d['text']\n",
    "tokens_ds = d['tokens_clean']\n",
    "labels_ds = d['labels']\n",
    "offset_mapping_init = d['offset_mapping_init']\n",
    "re_offset_mapping = d['re_offset_mapping']\n",
    "spacy_to_re = d['spacy_to_re']\n",
    "re_tokens = d['re_tokens']\n",
    "# offset_mapping_ds = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "# offset_mapping_ds1 = tokenize_with_spacy(full_text_ds)['offset_mapping']\n",
    "# offset_mapping_ds = strip_offset_mapping(full_text_ds,offset_mapping_ds)\n",
    "# idx,ds.df.iloc[idx]['nb_labels']\n",
    "len(tokens_ds),len(offset_mapping_init),len(re_tokens),len(re_offset_mapping) #len(offset_mapping_ds1),len(tokens_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8a09f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2fc78365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================== #\n",
    "def get_text_start_end(txt, s, search_from=0):\n",
    "    txt = txt[int(search_from):]\n",
    "    try:\n",
    "        idx = txt.find(s)\n",
    "        if idx >= 0:\n",
    "            st = idx\n",
    "            ed = st + len(s)\n",
    "        else:\n",
    "            raise ValueError('Error')\n",
    "    except:\n",
    "        res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "        if len(res):\n",
    "            st, ed = res[0][0], res[0][1]\n",
    "        else:\n",
    "            m = SequenceMatcher(None, s, txt).get_opcodes()\n",
    "            for tag, i1, i2, j1, j2 in m:\n",
    "                if tag == 'replace':\n",
    "                    s = s[:i1] + txt[j1:j2] + s[i2:]\n",
    "                if tag == \"delete\":\n",
    "                    s = s[:i1] + s[i2:]\n",
    "\n",
    "            res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "            if len(res):\n",
    "                st, ed = res[0][0], res[0][1]\n",
    "            else:\n",
    "                idx = txt.find(s)\n",
    "                if idx >= 0:\n",
    "                    st = idx\n",
    "                    ed = st + len(s)\n",
    "                else:\n",
    "                    st, ed = 0, 0\n",
    "    return st + search_from, ed + search_from\n",
    "    \n",
    "def get_offset_mapping(full_text, tokens):\n",
    "    offset_mapping = []\n",
    "\n",
    "    current_offset = 0\n",
    "    for token in tokens:\n",
    "        start, end = get_text_start_end(full_text, token, search_from=current_offset)\n",
    "        offset_mapping.append((start, end))\n",
    "        current_offset = end\n",
    "\n",
    "    return offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7845c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.df.iloc[idx]['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a36a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = df.iloc[idx]['full_text']\n",
    "tokens = df.iloc[idx]['tokens']\n",
    "labels = df.iloc[idx]['labels']\n",
    "offset_mapping = get_offset_mapping(full_text, tokens)\n",
    "# idx,df.iloc[idx]['nb_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b6c3179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jana: (6, 10) : B-NAME_STUDENT\n",
      "Telfah: (11, 17) : I-NAME_STUDENT\n",
      "nbarker@hotmail.com: (26, 45) : B-EMAIL\n",
      "(: (55, 56) : B-PHONE_NUM\n",
      "820)913: (56, 63) : I-PHONE_NUM\n",
      "-: (63, 64) : I-PHONE_NUM\n",
      "3241x894: (64, 72) : I-PHONE_NUM\n",
      "Jana: (2365, 2369) : B-NAME_STUDENT\n",
      "Telfah: (2370, 2376) : I-NAME_STUDENT\n",
      "nbarker@hotmail.com: (2385, 2404) : B-EMAIL\n",
      "(: (2414, 2415) : B-PHONE_NUM\n",
      "820)913: (2415, 2422) : I-PHONE_NUM\n",
      "-: (2422, 2423) : I-PHONE_NUM\n",
      "3241x894: (2423, 2431) : I-PHONE_NUM\n",
      "Jana: (2486, 2490) : B-NAME_STUDENT\n",
      "Telfah: (2491, 2497) : I-NAME_STUDENT\n",
      "nbarker@hotmail.com: (2506, 2525) : B-EMAIL\n",
      "(: (2535, 2536) : B-PHONE_NUM\n",
      "820)913: (2536, 2543) : I-PHONE_NUM\n",
      "-: (2543, 2544) : I-PHONE_NUM\n",
      "3241x894: (2544, 2552) : I-PHONE_NUM\n"
     ]
    }
   ],
   "source": [
    "offset_mapping = get_offset_mapping(full_text, tokens)\n",
    "for token, offset,l in zip(tokens, offset_mapping,labels):\n",
    "    if l!=\"O\":\n",
    "        print(f\"{token}: {offset} : {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a185a4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([695, 4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['word_boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9edb4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[665,   7,   1],\n",
       "        [666,   7,   1],\n",
       "        [667,   7,   1],\n",
       "        [668,   7,   1],\n",
       "        [669,   7,   1],\n",
       "        [670,   7,   1],\n",
       "        [671,   7,   1],\n",
       "        [672,   7,   1],\n",
       "        [673,   7,   1],\n",
       "        [674,   7,   1],\n",
       "        [675,   7,   1],\n",
       "        [676,   7,   1],\n",
       "        [677,   7,   1],\n",
       "        [678,   7,   1],\n",
       "        [679,   7,   1],\n",
       "        [680,   7,   1],\n",
       "        [681,   7,   1],\n",
       "        [682,   7,   1],\n",
       "        [683,   7,   1],\n",
       "        [684,   7,   1],\n",
       "        [685,   7,   1],\n",
       "        [686,   7,   1],\n",
       "        [687,   7,   1],\n",
       "        [688,   7,   1],\n",
       "        [689,   7,   1],\n",
       "        [690,   7,   1],\n",
       "        [691,   7,   1],\n",
       "        [692,   7,   1],\n",
       "        [693,   7,   1],\n",
       "        [694,   7,   1]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['gt_spans'][-30:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ef6bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Henry Email: william32@yahoo.com Mobile: (871)873-8640x14004 | Design Thinking For Innovation | Challenge & Selection Jordan’s economic growth and stability is strengthened by thriving entrepreneurial ecosystem focused on tech innovation and tech-based startups (TBS). While past efforts have improved ecosystem dynamics and innovation input; nonetheless, innovation outputs and socioeconomic impact remains marginal for many reasons, among which are inefficiencies, replication, lack of creativity, and low- quality pipeline. | Mid last year, UVA (University of Bedfordshire) in Jordan decided to contribute to this conversation and to the Jordanian tech ecosystem by establishing, a tech accelerator called (The Belvedere Vodka). I was chosen as the Program Manager for this project, and my current challenge was to develop new models that would steer the university and other educational institutions towards more responsible and value- focused models of entrepreneurial support. | We started with Mind Mapping to obtain insightful inputs at that phase after collecting large data by meeting, interviewing experts in the ecosystem individually, and taking them around for a tour in the facility. This allowed us to look into different patterns and insights during our exploration for founding The Belvedere Vodka accelerator. | Additionally, Journey Mapping was also implemented by interviewing tech-startups, which allowed us to put ourselves in their shoes, and experience the whole entrepreneurial journey, lack of support and need other models are not covering. | Application & Insight We have bought our own large white board to layout all the data and the notes picked out from our meetings. We started clustering and linking all the good notes on the board, which led us into getting major insights and finding out the main reason why the overall economic impact of other models remains marginal, and Jordan’s share of investments and successful exits remain low in comparison to inputs and the region. We believed this partly was due to: | 1. Inefficiencies in managing innovation input, high duplication, and low coordination among | ecosystem stakeholders. | https://instagram.com/russellscott 2. Ineffectiveness of tool based capacity-building programs. 3. Lack of focus on quality and requisite know-how of developing investment-ready tech-based | startups. | Name: Wilson Email: bradleyvilla@gmail.com Mobile: 418.445.3574 | Here is a glimpse of our white board mapping: | Name: Brown Email: brendacortez@hotmail.com Mobile: 001-265-770-8693 | Therefore, we proposed a rethinking of current acceleration models to the University board members, by developing vertically focused, innovation-driven, hybrid models that are dynamic, relevant and effective in delivering high quality investable and scalable tech-based startups. This would in turn support job creation, adoption of 21st century skills and know-how, increase mobilization of risk capital, and improve Jordan’s ranking in innovation & entrepreneurship. | Approach As previously mentioned, Mind and Journey Mapping were two successful design tools to be obtained by our team to access the “What is” phase for other models and the status of the startups in Jordan. Both tools helped us in getting insightful inputs and establishing solid criteria for the “What If” phase, which we are currently working on. | Therefore, the rational for future action for the model design stems from the below outcome three needs: | 1. Need to improve the quality and quantity of investment -ready companies within the Jordanian | tech innovation and ICT sectors. | 2. Need for novel and effective mechanisms, strategies and programs of incubation & acceleration | that significantly enhance investment-readiness and success of TBS. | 3. Need to seed and nurture the necessary mindset, skillset, and culture of innovation driven | entrepreneurship among Jordanian youth. |\n"
     ]
    }
   ],
   "source": [
    "# text = \"Reflection – Visualization   Deiby\"\n",
    "print((full_text_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f57e8f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_re</th>\n",
       "      <th>labels</th>\n",
       "      <th>spacy_to_re</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Name</td>\n",
       "      <td>Name</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Henry</td>\n",
       "      <td>Henry</td>\n",
       "      <td>NAME_STUDENT</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Email</td>\n",
       "      <td>Email</td>\n",
       "      <td>O</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>william32@yahoo.com</td>\n",
       "      <td>william32@yahoo.com</td>\n",
       "      <td>EMAIL</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mobile</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>O</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>PHONE_NUM</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tokens            tokens_re        labels  spacy_to_re\n",
       "0                 Name                 Name             O            0\n",
       "1                    :                    :             O            1\n",
       "2                Henry                Henry  NAME_STUDENT            2\n",
       "3                Email                Email             O            3\n",
       "4                    :                    :             O            4\n",
       "5  william32@yahoo.com  william32@yahoo.com         EMAIL            5\n",
       "6               Mobile               Mobile             O            6\n",
       "7                    :                    :             O            7\n",
       "8                    (                    (     PHONE_NUM            8"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame({'tokens':tokens_ds,'tokens_re':re_tokens,\n",
    "              'labels':labels_ds,'spacy_to_re':spacy_to_re})\n",
    "\n",
    "x[x.spacy_to_re<=8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d9a3eb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695, 1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.DataFrame({'spacy_to_re':d[\"spacy_to_re_unique\"]})\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0e0bc7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(695, 4)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec04e0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spacy_to_re</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_re</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [spacy_to_re, tokens, tokens_re, labels]\n",
       "Index: []"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = s.merge(x,how='left',on='spacy_to_re')\n",
    "s[s.tokens!=s.tokens_re]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d2b10295",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_mapping = get_offset_mapping(full_text, tokens)\n",
    "offset_mapping_ = [x for (x,y) in zip(offset_mapping,labels) if y!=\"O\"]\n",
    "labels_ = [x for x in labels if x!=\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1f270391",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Name: \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Telfah\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       "  Email: \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nbarker@hotmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       "  Mobile: \n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    820)913\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3241x894\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "<br><br>Design Thinking For Innovation<br><br>Challenge &amp; Selection  Jordan’s economic growth and stability is strengthened by thriving entrepreneurial ecosystem focused  on tech innovation and tech-based startups (TBS). While past efforts have improved ecosystem  dynamics and innovation input; nonetheless, innovation outputs and socioeconomic impact remains  marginal for many reasons, among which are inefficiencies, replication, lack of creativity, and low- quality pipeline.<br><br>Mid last year, UVA (University of Bedfordshire) in Jordan decided to contribute to this conversation  and to the Jordanian tech ecosystem by establishing, a tech accelerator called (The Belvedere Vodka). I was  chosen as the Program Manager for this project, and my current challenge was to develop new models  that would steer the university and other educational institutions towards more responsible and value- focused models of entrepreneurial support.<br><br>We started with Mind Mapping to obtain insightful inputs at that phase after collecting large data by  meeting, interviewing experts in the ecosystem individually, and taking them around for a tour in the  facility. This allowed us to look into different patterns and insights during our exploration for founding  The Belvedere Vodka accelerator.<br><br>Additionally, Journey Mapping was also implemented by interviewing tech-startups, which allowed us to  put ourselves in their shoes, and experience the whole entrepreneurial journey, lack of support and  need other models are not covering.<br><br>Application &amp; Insight  We have bought our own large white board to layout all the data and the notes picked out from our  meetings. We started clustering and linking all the good notes on the board, which led us into getting  major insights and finding out the main reason why the overall economic impact of other models  remains marginal, and Jordan’s share of investments and successful exits remain low in comparison to  inputs and the region. We believed this partly was due to:<br><br>1. Inefficiencies in managing innovation input, high duplication, and low coordination among<br><br>ecosystem stakeholders.<br><br>2. Ineffectiveness of tool based capacity-building programs.  3. Lack of focus on quality and requisite know-how of developing investment-ready tech-based<br><br>startups.<br><br>Name: \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Telfah\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       "  Email: \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nbarker@hotmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       "  Mobile: \n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    820)913\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3241x894\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "<br><br>Here is a glimpse of our white board mapping:<br><br>Name: \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Telfah\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       "  Email: \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nbarker@hotmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       "  Mobile: \n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    820)913\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3241x894\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "<br><br>Therefore, we proposed a rethinking of current acceleration models to the University board members,  by developing vertically focused, innovation-driven, hybrid models that are dynamic, relevant and  effective in delivering high quality investable and scalable tech-based startups. This would in turn  support job creation, adoption of 21st century skills and know-how, increase mobilization of risk capital,  and improve Jordan’s ranking in innovation &amp; entrepreneurship.<br><br>Approach  As previously mentioned, Mind and Journey Mapping were two successful design tools to be obtained  by our team to access the “What is” phase for other models and the status of the startups in Jordan.  Both tools helped us in getting insightful inputs and establishing solid criteria for the “What If” phase,  which we are currently working on.<br><br>Therefore, the rational for future action for the model design stems from the below outcome three  needs:<br><br>1. Need to improve the quality and quantity of investment -ready companies within the Jordanian<br><br>tech innovation and ICT sectors.<br><br>2. Need for novel and effective mechanisms, strategies and programs of incubation &amp; acceleration<br><br>that significantly enhance investment-readiness and success of TBS.<br><br>3. Need to seed and nurture the necessary mindset, skillset, and culture of innovation driven<br><br>entrepreneurship among Jordanian youth.<br><br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(full_text,offset_mapping_,labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7dc54c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset_mapping = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "offset_mapping_ = [x for (x,y) in zip(offset_mapping_init,labels_ds) if y!=\"O\"]\n",
    "labels_ = [x for x in labels_ds if x!=\"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f05249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (x,y) in zip(offset,labels_ds):\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7ec06cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Name: \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Henry\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " Email: \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    william32@yahoo.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       " Mobile: \n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    871)873\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    8640x14004\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       " | Design Thinking For Innovation | Challenge &amp; Selection Jordan’s economic growth and stability is strengthened by thriving entrepreneurial ecosystem focused on tech innovation and tech-based startups (TBS). While past efforts have improved ecosystem dynamics and innovation input; nonetheless, innovation outputs and socioeconomic impact remains marginal for many reasons, among which are inefficiencies, replication, lack of creativity, and low- quality pipeline. | Mid last year, UVA (University of Bedfordshire) in Jordan decided to contribute to this conversation and to the Jordanian tech ecosystem by establishing, a tech accelerator called (The Belvedere Vodka). I was chosen as the Program Manager for this project, and my current challenge was to develop new models that would steer the university and other educational institutions towards more responsible and value- focused models of entrepreneurial support. | We started with Mind Mapping to obtain insightful inputs at that phase after collecting large data by meeting, interviewing experts in the ecosystem individually, and taking them around for a tour in the facility. This allowed us to look into different patterns and insights during our exploration for founding The Belvedere Vodka accelerator. | Additionally, Journey Mapping was also implemented by interviewing tech-startups, which allowed us to put ourselves in their shoes, and experience the whole entrepreneurial journey, lack of support and need other models are not covering. | Application &amp; Insight We have bought our own large white board to layout all the data and the notes picked out from our meetings. We started clustering and linking all the good notes on the board, which led us into getting major insights and finding out the main reason why the overall economic impact of other models remains marginal, and Jordan’s share of investments and successful exits remain low in comparison to inputs and the region. We believed this partly was due to: | 1. Inefficiencies in managing innovation input, high duplication, and low coordination among | ecosystem stakeholders. | \n",
       "<mark class=\"entity\" style=\"background: #ff8042; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    https://instagram.com/russellscott\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">URL_PERSONAL</span>\n",
       "</mark>\n",
       " 2. Ineffectiveness of tool based capacity-building programs. 3. Lack of focus on quality and requisite know-how of developing investment-ready tech-based | startups. | Name: \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wilson\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " Email: \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bradleyvilla@gmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       " Mobile: \n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    418.445.3574\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       " | Here is a glimpse of our white board mapping: | Name: \n",
       "<mark class=\"entity\" style=\"background: #8000ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brown\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME_STUDENT</span>\n",
       "</mark>\n",
       " Email: \n",
       "<mark class=\"entity\" style=\"background: #2b7ff6; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    brendacortez@hotmail.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
       "</mark>\n",
       " Mobile: \n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    001\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    265\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    770\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    -\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    8693\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       " | Therefore, we proposed a rethinking of current acceleration models to the University board members, by developing vertically focused, innovation-driven, hybrid models that are dynamic, relevant and effective in delivering high quality investable and scalable tech-based startups. This would in turn support job creation, adoption of 21st century skills and know-how, increase mobilization of risk capital, and improve Jordan’s ranking in innovation &amp; entrepreneurship. | Approach As previously mentioned, Mind and Journey Mapping were two successful design tools to be obtained by our team to access the “What is” phase for other models and the status of the startups in Jordan. Both tools helped us in getting insightful inputs and establishing solid criteria for the “What If” phase, which we are currently working on. | Therefore, the rational for future action for the model design stems from the below outcome three needs: | 1. Need to improve the quality and quantity of investment -ready companies within the Jordanian | tech innovation and ICT sectors. | 2. Need for novel and effective mechanisms, strategies and programs of incubation &amp; acceleration | that significantly enhance investment-readiness and success of TBS. | 3. Need to seed and nurture the necessary mindset, skillset, and culture of innovation driven | entrepreneurship among Jordanian youth. |</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(full_text_ds,offset_mapping_,labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16b37dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "en_tokenizer = English().tokenizer\n",
    "import re\n",
    "try:\n",
    "    from faker import Faker\n",
    "    fake = Faker(locale = [\"fr_FR\",\"fr_CA\",\"en_US\",'en_UK','de_DE','en_GB','en_IN','it_IT','fr_BE'])\n",
    "#     Faker.seed(0)\n",
    "except:\n",
    "    print('No faker installed')\n",
    "    \n",
    "try:\n",
    "    from spacy.lang.en import English\n",
    "    EN_TOK = English().tokenizer\n",
    "except:\n",
    "    print(\"No spacy\")\n",
    "    \n",
    "\n",
    "def tokenize_with_spacy(text, tokenizer=en_tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = [token.text for token in tokenized_text]\n",
    "    offset_mapping = [(token.idx,token.idx+len(token)) for token in tokenized_text]\n",
    "    return {'tokens': tokens, 'offset_mapping': offset_mapping}\n",
    "\n",
    "\n",
    "def remove_double_spaces(text):\n",
    "    # Use a regular expression to replace consecutive spaces with a single space\n",
    "    # cleaned_text = re.sub(r'\\s{2,}', ' | ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(u'\\x9d', u' ')\n",
    "    # text = resolve_encodings_and_normalize(text)\n",
    "    # text = text.replace(u'\\xa0', u' ')\n",
    "    # text = text.replace(u'\\x85', u'\\n')\n",
    "    text = remove_double_spaces(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# ======================================================================================== #\n",
    "def find_successive_numbers(input_array):\n",
    "    result = []\n",
    "    current_sublist = []\n",
    "\n",
    "    for num in input_array:\n",
    "        if not current_sublist or num == current_sublist[-1] + 1:\n",
    "            current_sublist.append(num)\n",
    "        else:\n",
    "            result.append(current_sublist)\n",
    "            current_sublist = [num]\n",
    "\n",
    "    if current_sublist:\n",
    "        result.append(current_sublist)\n",
    "\n",
    "    return result\n",
    "# ======================================================================================== #\n",
    "\n",
    "def generate_random_number(length):\n",
    "    return ''.join(random.choice('0123456789') for _ in range(length))\n",
    "# ======================================================================================== #\n",
    "def generate_fake_social_media_urls(num_urls=1):\n",
    "    social_media_platforms = {\n",
    "        'LinkedIn': 'linkedin.com/in/',\n",
    "        'YouTube': 'youtube.com/c/',\n",
    "        'Instagram': 'instagram.com/',\n",
    "        'GitHub': 'github.com/',\n",
    "        'Facebook': 'facebook.com/',\n",
    "        'Twitter': 'twitter.com/'\n",
    "    }\n",
    "\n",
    "    fake_social_media_urls = []\n",
    "\n",
    "    for _ in range(num_urls):\n",
    "        fake_user_name = fake.user_name()\n",
    "        platform, domain = random.choice(list(social_media_platforms.items()))\n",
    "        fake_url = f'https://{domain}{fake_user_name}'\n",
    "        fake_social_media_urls.append(fake_url)\n",
    "\n",
    "    return fake_social_media_urls[0]\n",
    "    \n",
    "# ======================================================================================== #\n",
    "def generate_random_data_with_probabilities():\n",
    "\n",
    "    name = random.choices([fake.name(),fake.first_name(), fake.last_name()],\n",
    "                          weights = [0.7,0.15,0.15], k = 1)[0]  #generic.person.full_name()\n",
    "    phone_number =  fake.phone_number()\n",
    "    username = fake.user_name()\n",
    "    email = fake.ascii_free_email()\n",
    "    address = fake.address()\n",
    "    id_num = random.choices([fake.passport_number(),fake.bban(), fake.iban(),\n",
    "                             fake.credit_card_number(),fake.swift(),fake.ean(),\n",
    "                             generate_random_number(12)],k=1,weights = [1/7]*7)[0]\n",
    "    url_pers = generate_fake_social_media_urls()\n",
    "\n",
    "    ret = dict(\n",
    "              NAME_STUDENT=name,\n",
    "              EMAIL=email,\n",
    "              USERNAME=username,\n",
    "              ID_NUM=id_num,\n",
    "              URL_PERSONAL=url_pers,\n",
    "              PHONE_NUM=phone_number,\n",
    "              STREET_ADDRESS=address\n",
    "              )\n",
    "\n",
    "    for k,v in ret.items():\n",
    "        ret[k] = clean_text(v.replace(\"\\n\\n\",\" | \").replace(\"\\n\",\" [BR] \"))\n",
    "    return ret\n",
    "# ======================================================================================== #\n",
    "def name_student(v):\n",
    "    # Add Name for Name/ and Mobile/Tel for phone Email for mail\n",
    "    text = random.choices([f\"Reflection – Visualization {v}\",f'Person {v}',\n",
    "                   f\"STORYTELLER {v}\",f\"STORY TELLING {v}\",f\"{v}\"],k=1,weights = [0.125,0.125,0.125,0.125,0.5])[0]\n",
    "    \n",
    "    return text\n",
    "# ======================================================================================== #\n",
    "def generate_fake_data():\n",
    "    \n",
    "    data = generate_random_data_with_probabilities()\n",
    "    \n",
    "    NB_PII_MAX = random.choice([1,2,3])\n",
    "    piis_ent = random.sample(list(data.keys()),k=NB_PII_MAX)\n",
    "    \n",
    "    print(piis_ent)\n",
    "    \n",
    "    full_text = \"\"\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    offset_mapping = []\n",
    "    off = 0\n",
    "    for num, ent in enumerate(piis_ent):\n",
    "#         print(ent)\n",
    "        if ent==\"NAME_STUDENT\":\n",
    "            text = name_student(data[ent])\n",
    "            if num ==0:\n",
    "                full_text = text\n",
    "            else:\n",
    "                off = off + 1 \n",
    "                full_text = full_text + \" \" + text\n",
    "                \n",
    "            toks = tokenize_with_spacy(text)\n",
    "            tokns = toks['tokens']\n",
    "            offset = toks['offset_mapping']\n",
    "            labs = np.array([\"O\"]*len(tokns),dtype='<U50')\n",
    "            idx = [i for i,x in enumerate(tokns) if x in data[ent]][0]\n",
    "            labs[idx:] = \"NAME_STUDENT\"\n",
    "            \n",
    "            tokens = tokens + tokns\n",
    "            labels = labels + labs.tolist()\n",
    "            new_offset = [(x[0]+off,x[1]+off) for x in offset]\n",
    "            offset_mapping = offset_mapping + new_offset\n",
    "            off = offset_mapping[-1][1]\n",
    "#             print(off)\n",
    "        else:\n",
    "            text = data[ent]\n",
    "            \n",
    "            if num ==0:\n",
    "                full_text = text\n",
    "            else:\n",
    "                off = off+1\n",
    "                full_text = full_text + \" \" + text\n",
    "            \n",
    "            toks = tokenize_with_spacy(text)\n",
    "            tokns = toks['tokens']\n",
    "            offset = toks['offset_mapping']\n",
    "            labs = [ent]*len(tokns)\n",
    "            \n",
    "            tokens = tokens + tokns\n",
    "            labels = labels + labs\n",
    "            \n",
    "            new_offset = [(x[0]+off,x[1]+off) for x in offset]\n",
    "            offset_mapping = offset_mapping + new_offset\n",
    "            off = offset_mapping[-1][1]\n",
    "#             print(off)\n",
    "#         print(labels)\n",
    "    return full_text,tokens,labels,offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e4e9fb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tatiana Pezzali-Battisti'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "28937cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('G92491893',\n",
       " '959245225788873344',\n",
       " 'GB40MWTL93332316312216',\n",
       " '676333923347',\n",
       " 'NUAGFROZFI3',\n",
       " '6003834602338')"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.passport_number(),fake.bban(), fake.iban(), fake.credit_card_number(),fake.swift(),fake.ean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.license_plate(),fake.ssn(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "d8a7f6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b_bhu$(x5S9Z'"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.password(length=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "e61b89c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NAME_STUDENT': 'Vaillant',\n",
       " 'EMAIL': 'apostolostrommler@yahoo.de',\n",
       " 'USERNAME': 'massartmatteo',\n",
       " 'ID_NUM': 'ZKCY5351271851823',\n",
       " 'URL_PERSONAL': 'https://youtube.com/c/dianacross',\n",
       " 'PHONE_NUM': '028 9018901',\n",
       " 'STREET_ADDRESS': 'Buchholzstr. 4 [BR] 37480 Erkelenz'}"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = generate_random_data_with_probabilities()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "6411bcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vaillant'"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = name_student(data['NAME_STUDENT'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "b8101a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Vaillant'], 'offset_mapping': [(0, 8)]}"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_spacy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "e673339b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NAME_STUDENT', 'URL_PERSONAL']"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_PII_MAX = random.choice([1,2,3])\n",
    "random.sample(list(data.keys()),k=NB_PII_MAX,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "6070c17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PHONE_NUM', 'STREET_ADDRESS']\n"
     ]
    }
   ],
   "source": [
    "full_text,tokens,labels,offset_mapping = generate_fake_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "bd6f4877",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: d4dd80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    +919789633759\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE_NUM</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    728\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Murphy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    square\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Harrietville\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    [\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ]\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    L31\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ff0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    8QP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# offset_mapping = get_offset_mapping(full_text_ds, tokens_ds)\n",
    "offset_mapping_ = [x for (x,y) in zip(offset_mapping,labels) if y!=\"O\"]\n",
    "labels_ = [x for x in labels if x!=\"O\"]\n",
    "visualize(full_text,offset_mapping_,labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "38532ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+919789633759 728 Murphy square [BR] Harrietville [BR] L31 8QP'"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "d45dc21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 13),\n",
       " (14, 17),\n",
       " (18, 24),\n",
       " (25, 31),\n",
       " (32, 33),\n",
       " (33, 35),\n",
       " (35, 36),\n",
       " (37, 49),\n",
       " (50, 51),\n",
       " (51, 53),\n",
       " (53, 54),\n",
       " (55, 58),\n",
       " (59, 62)]"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "463bd212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PHONE_NUM', 'USERNAME', 'EMAIL']"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "09d22cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+49(0) 067910765'"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.phone_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8aadb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c414b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import torch\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "# from difflib import SequenceMatcher\n",
    "\n",
    "# import codecs\n",
    "# import os\n",
    "# from collections import Counter\n",
    "# from typing import Dict, List, Tuple\n",
    "\n",
    "# from text_unidecode import unidecode\n",
    "# import joblib\n",
    "# import torch\n",
    "\n",
    "\n",
    "# try:\n",
    "#     from faker import Faker\n",
    "#     fake = Faker()\n",
    "#     Faker.seed(0)\n",
    "# except:\n",
    "#     print('No faker installed')\n",
    "    \n",
    "# try:\n",
    "#     from spacy.lang.en import English\n",
    "#     EN_TOK = English().tokenizer\n",
    "# except:\n",
    "#     print(\"No spacy\")\n",
    "\n",
    "    \n",
    "    \n",
    "# def process_regex(pattern, reverse=False):\n",
    "#     replacements = {\n",
    "#         '(': r'\\(',\n",
    "#         ')': r'\\)',\n",
    "#         '[': r'\\[',\n",
    "#         ']': r'\\]',\n",
    "#         '|': r'\\|',\n",
    "#         '?': r'\\?',\n",
    "#         '*': r'\\*',\n",
    "#         '+': r'\\+'\n",
    "#     }\n",
    "    \n",
    "#     if reverse:\n",
    "#         replacements = {v: k for k, v in replacements.items()}\n",
    "    \n",
    "#     for old, new in replacements.items():\n",
    "#         pattern = pattern.replace(old, new)\n",
    "    \n",
    "#     return pattern\n",
    "\n",
    "\n",
    "\n",
    "# LABEL2TYPE = ('NAME_STUDENT','EMAIL','USERNAME','ID_NUM', 'PHONE_NUM','URL_PERSONAL','STREET_ADDRESS','O')\n",
    "# TYPE2LABEL = {t: l for l, t in enumerate(LABEL2TYPE)}\n",
    "# ID_TYPE = {\"0-0\":0,\"0-1\":1,\n",
    "#            \"1-0\":2,\"1-1\":3,\n",
    "#            \"2-0\":4,\"2-1\":5,\n",
    "#            \"3-0\":6,\"3-1\":7,\n",
    "#            \"4-0\":8,\"4-1\":9,\n",
    "#            \"5-0\":10,\"5-1\":11,\n",
    "#            \"6-0\":12,\"6-1\":13\n",
    "#           }\n",
    "\n",
    "# ID_NAME = {\"0-0\":\"B-NAME_STUDENT\",\"0-1\":\"I-NAME_STUDENT\",\n",
    "#            \"1-0\":\"B-EMAIL\",\"1-1\":\"I-EMAIL\",\n",
    "#            \"2-0\":\"B-USERNAME\",\"2-1\":\"I-USERNAME\",\n",
    "#            \"3-0\":\"B-ID_NUM\",\"3-1\":\"I-ID_NUM\",\n",
    "#            \"4-0\":\"B-PHONE_NUM\",\"4-1\":\"I-PHONE_NUM\",\n",
    "#            \"5-0\":\"B-URL_PERSONAL\",\"5-1\":\"I-URL_PERSONAL\",\n",
    "#            \"6-0\":\"B-STREET_ADDRESS\",\"6-1\":\"I-STREET_ADDRESS\",\n",
    "#            \"7-0\":\"O\",\"7-1\":\"O\"\n",
    "#           }\n",
    "\n",
    "# RE_ID_PHONE = r\"\"\"(\\(?\\+\\s*\\d{1,4}\\s*\\)?\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\s{0,2}\\d{0,5}|\\(?\\+\\s*\\d{1,4}\\s*\\)?\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s{0,2}\\d{0,5}|\\(?\\s*\\d{1,4}\\s*\\)?\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s{0,2}\\d{0,5}\\s*[\\.\\-x]?\\d{1,5}\\s{0,2}\\d{0,5}|\\(?\\s*\\d{1,4}\\s*\\)?\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s*[\\.\\-x]?\\d{1,5}\\s{0,2}\\d{0,5}|\\(\\s*\\d{3}\\s*\\)\\s*\\d{3}\\s*\\-\\s*\\d{4}\\s*\\w{0,3}(\\s*\\d{1,8}\\s*)?|\\b\\d{2,}-\\d{2,}\\.\\d{2,}\\.\\d{2,}\\.\\d{2,}\\b|\\b\\d{2,}-\\d{2,}\\-\\d{2,}\\-\\d{2,}\\-\\d{2,}\\b|\\b\\d{2,}\\-\\d{2,}\\-\\d{2,}\\-\\d{2,}\\b|\\b\\d{2,}\\.\\d{2,}\\.\\d{2,}\\.\\d{2,}\\b|\\d{3}\\s*\\.\\s*\\d{3}\\s*\\.\\s*\\d{1,5}|\\d{3}\\s*\\-\\s*\\d{3}\\s*\\-\\s*\\d{1,5}|\\d{3}\\s*x\\s*\\d{3}\\s*x\\s*\\d{1,5}|\\d{1,3}\\s{0,2}\\d{1,}\\s{0,2}\\d{1,}|\\b\\d{1,}\\s*\\d{1,}\\s*\\d{1,}|\\b\\d{2,}\\-\\d{2,}\\-\\d{2,}\\b|\\b\\d{2,}\\.\\d{2,}\\.\\d{2,}\\b|\\b\\d{1,}-\\d{1,}|[\\w\\.\\:\\-\\_\\|]*\\d{6,})\"\"\"\n",
    "# REGEX_COMPILE = re.compile(RE_ID_PHONE)\n",
    "\n",
    "\n",
    "# ## =============================================================================== ##\n",
    "# class TrainDataset(Dataset):\n",
    "#     def __init__(self,\n",
    "#                  df,\n",
    "#                  tokenizer,\n",
    "#                  add_text_prob=0.9,\n",
    "#                  replace_text_prob=0.5,\n",
    "#                  attrib_to_replace = ['NAME_STUDENT','EMAIL','USERNAME','ID_NUM',\n",
    "#                                       'PHONE_NUM','URL_PERSONAL','STREET_ADDRESS']\n",
    "#                  ):\n",
    "        \n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.df = df\n",
    "#         self.attrib_to_replace = attrib_to_replace\n",
    "\n",
    "#         print(f'Loaded {len(self)} samples.')\n",
    "\n",
    "#         assert 0 <= add_text_prob <= 1\n",
    "#         assert 0 <= replace_text_prob <= 1\n",
    "#         self.add_text_prob = add_text_prob\n",
    "#         self.replace_text_prob = replace_text_prob\n",
    "#     # ======================================================================================== #\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "#     # ======================================================================================== #\n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         df = self.df.iloc[index]\n",
    "#         text_id = df['document']\n",
    "        \n",
    "#         ## Adding space tokens\n",
    "#         if len(self.tokenizer.encode(\"\\n\\n\"))==2:\n",
    "#             text = self.clean_text(df['full_text'].replace(\"\\n\\n\",\" | \").replace(\"\\n\",\" [BR] \"))\n",
    "#             txt_tokens = [self.clean_text(x.replace(\"\\n\\n\",\" | \").replace(\"\\n\",\" [BR] \")) for x in df['tokens']]\n",
    "#         else:\n",
    "#             text = self.clean_text(df['full_text'])\n",
    "#             txt_tokens = [self.clean_text(x) for x in df['tokens']]\n",
    "            \n",
    "#         labels = df['labels']\n",
    "#         offset_mapping_init = self.get_offset_mapping(text,txt_tokens)\n",
    "        \n",
    "#         ##############   Augmentation   #############\n",
    "#         # Replace PII\n",
    "# #         prob = \n",
    "#         if np.random.random()< self.replace_text_prob:\n",
    "#             print(\"rep\")\n",
    "#             text,labels,txt_tokens = self.create_mapper_n_clean(text,labels,offset_mapping_init,\n",
    "#                                                                        attribut=self.attrib_to_replace)\n",
    "#             offset_mapping_init = self.get_offset_mapping(text, txt_tokens)\n",
    "        \n",
    "\n",
    "    \n",
    "#         # Add PII\n",
    "#         if np.random.random() < self.add_text_prob:\n",
    "#             print(\"add\")\n",
    "#             new_text,new_tokens,new_labels,new_offset_mapping = self.generate_fake_data()\n",
    "            \n",
    "        \n",
    "            \n",
    "#             text,txt_tokens,labels,offset_mapping_init  = self.add_text(text,txt_tokens,labels,offset_mapping_init,\n",
    "#                                                                    new_text,new_tokens,new_labels,new_offset_mapping)\n",
    "        \n",
    "#         ### Convert to regex space\n",
    "#         re_offset_mapping,spacy_to_re,re_tokens,spacy_to_re_unique = self.spacy_to_re_off(text,txt_tokens,offset_mapping_init)\n",
    "\n",
    "\n",
    "#         hf_tokens = self.tokenizer(text, return_offsets_mapping=True)\n",
    "#         input_ids = torch.LongTensor(hf_tokens['input_ids'])\n",
    "#         attention_mask = torch.LongTensor(hf_tokens['attention_mask'])\n",
    "#         hf_offset_mapping = np.array(hf_tokens['offset_mapping'])\n",
    "\n",
    "#         num_tokens = len(input_ids)\n",
    "\n",
    "#         # token slices of words\n",
    "#         woff = np.array(re_offset_mapping)\n",
    "#         toff = np.array(hf_offset_mapping)\n",
    "#         wx1, wx2 = woff.T\n",
    "#         tx1, tx2 = toff.T\n",
    "#         ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n",
    "#         ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n",
    "#         ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n",
    "#         ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n",
    "#         ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1)\n",
    "# #         assert (ious > 0).any(-1).all()\n",
    "\n",
    "#         word_boxes = []\n",
    "# #         err = []\n",
    "#         for i,row in enumerate(ious):\n",
    "#             inds = row.nonzero()[0]\n",
    "#             try:\n",
    "#                 word_boxes.append([inds[0], 0, inds[-1] + 1, 1])\n",
    "#             except:\n",
    "#                 word_boxes.append([-100, 0, -99, 1])\n",
    "# #                 err.append(i)\n",
    "                \n",
    "#         word_boxes = torch.FloatTensor(word_boxes)\n",
    "\n",
    "\n",
    "#         gt_spans = []        \n",
    "#         for i,label in enumerate(labels) :\n",
    "#             gt_spans.append([i,TYPE2LABEL[label.split('-')[-1] if label!=\"O\" else \"O\"],0 if label.split('-')[0]==\"B\" else 1])\n",
    "            \n",
    "#         gt_spans = torch.LongTensor(gt_spans)\n",
    "\n",
    "#         # random mask augmentation\n",
    "# #         if np.random.random() < self.mask_prob:\n",
    "# #             all_inds = np.arange(1, len(input_ids) - 1)\n",
    "# #             n_mask = max(int(len(all_inds) * self.mask_ratio), 1)\n",
    "# #             np.random.shuffle(all_inds)\n",
    "# #             mask_inds = all_inds[:n_mask]\n",
    "# #             input_ids[mask_inds] = self.tokenizer.mask_token_id\n",
    "\n",
    "#         return dict(\n",
    "#                     text_id=text_id,\n",
    "#                     text=text,\n",
    "#                     labels = labels,\n",
    "#                     re_tokens = re_tokens,\n",
    "#                     spacy_to_re = spacy_to_re,\n",
    "#                     spacy_to_re_unique = spacy_to_re_unique,\n",
    "#                     tokens = df['tokens'],\n",
    "#                     tokens_clean = txt_tokens,\n",
    "#                     input_ids=input_ids,\n",
    "#                     offset_mapping_init = offset_mapping_init,\n",
    "#                     re_offset_mapping = re_offset_mapping,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     word_boxes=word_boxes,\n",
    "#                     gt_spans=gt_spans)\n",
    "#     # ======================================================================================== #\n",
    "#     def name_student(self,v):\n",
    "#         # Add Name for Name/ and Mobile/Tel for phone Email for mail\n",
    "#         text = random.choices([f\"Reflection – Visualization {v}\",f'Person {v}',\n",
    "#                        f\"STORYTELLER {v}\",f\"STORY TELLING {v}\",f\"{v}\"],k=1,weights = [0.125,0.125,0.125,0.125,0.5])[0]\n",
    "\n",
    "#         return text\n",
    "#     # ======================================================================================== #\n",
    "#     def generate_fake_data(self):\n",
    "\n",
    "#         data = self.generate_random_data_with_probabilities()\n",
    "\n",
    "#         NB_PII_MAX = random.choice([1,2,3])\n",
    "#         piis_ent = random.sample(list(data.keys()),k=NB_PII_MAX)\n",
    "\n",
    "\n",
    "#         full_text = \"\"\n",
    "#         tokens = []\n",
    "#         labels = []\n",
    "#         offset_mapping = []\n",
    "#         off = 0\n",
    "#         for num, ent in enumerate(piis_ent):\n",
    "\n",
    "#             if ent==\"NAME_STUDENT\":\n",
    "#                 text = self.name_student(data[ent])\n",
    "#                 if num ==0:\n",
    "#                     full_text = text\n",
    "#                 else:\n",
    "#                     off = off + 1 \n",
    "#                     full_text = full_text + \" \" + text\n",
    "\n",
    "#                 toks = self.tokenize_with_spacy(text)\n",
    "#                 tokns = toks['tokens']\n",
    "#                 offset = toks['offset_mapping']\n",
    "#                 labs = np.array([\"O\"]*len(tokns),dtype='<U50')\n",
    "#                 idx = [i for i,x in enumerate(tokns) if x in data[ent]][0]\n",
    "#                 labs[idx:] = \"NAME_STUDENT\"\n",
    "\n",
    "#                 tokens = tokens + tokns\n",
    "#                 labels = labels + labs.tolist()\n",
    "#                 new_offset = [(x[0]+off,x[1]+off) for x in offset]\n",
    "#                 offset_mapping = offset_mapping + new_offset\n",
    "#                 off = offset_mapping[-1][1]\n",
    "\n",
    "#             else:\n",
    "#                 text = data[ent]\n",
    "\n",
    "#                 if num ==0:\n",
    "#                     full_text = text\n",
    "#                 else:\n",
    "#                     off = off+1\n",
    "#                     full_text = full_text + \" \" + text\n",
    "\n",
    "#                 toks = self.tokenize_with_spacy(text)\n",
    "#                 tokns = toks['tokens']\n",
    "#                 offset = toks['offset_mapping']\n",
    "#                 labs = [ent]*len(tokns)\n",
    "\n",
    "#                 tokens = tokens + tokns\n",
    "#                 labels = labels + labs\n",
    "\n",
    "#                 new_offset = [(x[0]+off,x[1]+off) for x in offset]\n",
    "#                 offset_mapping = offset_mapping + new_offset\n",
    "#                 off = offset_mapping[-1][1]\n",
    "\n",
    "#         return full_text,tokens,labels,offset_mapping\n",
    "\n",
    "#     def remove_double_spaces(self,text):\n",
    "#         # Use a regular expression to replace consecutive spaces with a single space\n",
    "#         # cleaned_text = re.sub(r'\\s{2,}', ' | ', text)\n",
    "#         cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "#         return cleaned_text\n",
    "\n",
    "#     def clean_text(self,text):\n",
    "#         text = text.replace(u'\\x9d', u' ')\n",
    "#         # text = resolve_encodings_and_normalize(text)\n",
    "#         # text = text.replace(u'\\xa0', u' ')\n",
    "#         # text = text.replace(u'\\x85', u'\\n')\n",
    "#         text = self.remove_double_spaces(text)\n",
    "#         text = text.strip()\n",
    "#         return text\n",
    "\n",
    "#     # ======================================================================================== #\n",
    "#     def find_successive_numbers(self,input_array):\n",
    "#         result = []\n",
    "#         current_sublist = []\n",
    "\n",
    "#         for num in input_array:\n",
    "#             if not current_sublist or num == current_sublist[-1] + 1:\n",
    "#                 current_sublist.append(num)\n",
    "#             else:\n",
    "#                 result.append(current_sublist)\n",
    "#                 current_sublist = [num]\n",
    "\n",
    "#         if current_sublist:\n",
    "#             result.append(current_sublist)\n",
    "\n",
    "#         return result\n",
    "#     # ======================================================================================== #\n",
    "\n",
    "#     def generate_random_number(self,length):\n",
    "#         return ''.join(random.choice('0123456789') for _ in range(length))\n",
    "#     # ======================================================================================== #\n",
    "#     def generate_fake_social_media_urls(self,num_urls=1):\n",
    "#         social_media_platforms = {\n",
    "#             'LinkedIn': 'linkedin.com/in/',\n",
    "#             'YouTube': 'youtube.com/c/',\n",
    "#             'Instagram': 'instagram.com/',\n",
    "#             'GitHub': 'github.com/',\n",
    "#             'Facebook': 'facebook.com/',\n",
    "#             'Twitter': 'twitter.com/'\n",
    "#         }\n",
    "\n",
    "#         fake_social_media_urls = []\n",
    "\n",
    "#         for _ in range(num_urls):\n",
    "#             fake_user_name = fake.user_name()\n",
    "#             platform, domain = random.choice(list(social_media_platforms.items()))\n",
    "#             fake_url = f'https://{domain}{fake_user_name}'\n",
    "#             fake_social_media_urls.append(fake_url)\n",
    "\n",
    "#         return fake_social_media_urls[0]\n",
    "#     # ======================================================================================== #\n",
    "#     def generate_random_data_with_probabilities(self):\n",
    "\n",
    "#         name = random.choices([fake.name(),fake.first_name(), fake.last_name()],\n",
    "#                               weights = [0.7,0.15,0.15], k = 1)[0]  #generic.person.full_name()\n",
    "#         phone_number =  fake.phone_number()\n",
    "#         username = fake.user_name()\n",
    "#         email = fake.ascii_free_email()\n",
    "#         address = fake.address()\n",
    "#         id_num = random.choices([fake.passport_number(),fake.bban(),\n",
    "#                                  fake.iban(),self.generate_random_number(12)],k=1,weights = [0.1,0.10,0.15,0.65])[0]\n",
    "#         url_pers = self.generate_fake_social_media_urls()\n",
    "\n",
    "#         ret = dict(\n",
    "#                   NAME_STUDENT=name,\n",
    "#                   EMAIL=email,\n",
    "#                   USERNAME=username,\n",
    "#                   ID_NUM=id_num,\n",
    "#                   URL_PERSONAL=url_pers,\n",
    "#                   PHONE_NUM=phone_number,\n",
    "#                   STREET_ADDRESS=address\n",
    "#                   )\n",
    "\n",
    "#         for k,v in ret.items():\n",
    "#             ret[k] = self.clean_text(v.replace(\"\\n\\n\",\" | \").replace(\"\\n\",\" [BR] \"))\n",
    "#         return ret\n",
    "#     # ======================================================================================== #\n",
    "#     def generate_ent(self,text,labels,offset_mapping):\n",
    "\n",
    "#         idx_lab = np.argwhere(np.array(labels)!=\"O\").reshape(-1)\n",
    "#         pos = self.find_successive_numbers(idx_lab)\n",
    "#         lab = np.array(labels)\n",
    "\n",
    "\n",
    "#         ent = {}\n",
    "#         ent_order = []\n",
    "#         ent_offset_in_order = []\n",
    "#         for i,p in enumerate(pos):\n",
    "#             l = [x.split('-')[-1] for x in lab[p]]\n",
    "\n",
    "#             if len(np.unique(l))>1:\n",
    "#                 px = self.successive_positions(l)\n",
    "#                 for pp in px:\n",
    "#                     full_name = text[offset_mapping[p[pp[0]]][0]:offset_mapping[p[pp[-1]]][1]].strip()\n",
    "#                     ent[full_name] = l[pp[-1]]\n",
    "#                     ent_order.append(full_name)\n",
    "#                     ent_offset_in_order.append((offset_mapping[p[pp[0]]][0],\n",
    "#                                                 offset_mapping[p[pp[-1]]][1]))\n",
    "\n",
    "#             else:\n",
    "#                 full_name = text[offset_mapping[p[0]][0]:offset_mapping[p[-1]][1]].strip()\n",
    "#                 ent[full_name] = l[-1]\n",
    "#                 ent_order.append(full_name)\n",
    "#                 ent_offset_in_order.append((offset_mapping[p[0]][0],offset_mapping[p[-1]][1]))\n",
    "\n",
    "#         return ent,ent_order,ent_offset_in_order\n",
    "#     # ======================================================================================== #\n",
    "#     def tokenize_with_spacy(self,text,tok=EN_TOK):\n",
    "#         tokenized_text = tok(text)\n",
    "#         tokens = [token.text for token in tokenized_text]\n",
    "#         offset_mapping = [(token.idx,token.idx+len(token)) for token in tokenized_text]\n",
    "#         return {'tokens': tokens, 'offset_mapping': offset_mapping}\n",
    "    \n",
    "#     def successive_positions(self,input_list):\n",
    "#         result = []\n",
    "#         current_group = []\n",
    "#         prev_element = None\n",
    "\n",
    "#         for i, element in enumerate(input_list):\n",
    "#             if element == prev_element:\n",
    "#                 current_group.append(i)\n",
    "#             else:\n",
    "#                 if current_group:\n",
    "#                     result.append(current_group)\n",
    "#                 current_group = [i]\n",
    "#             prev_element = element\n",
    "\n",
    "#         if current_group:\n",
    "#             result.append(current_group)\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     # ======================================================================================== #\n",
    "#     def get_offset_mapping(self,full_text, tokens):\n",
    "#         offset_mapping = []\n",
    "\n",
    "#         current_offset = 0\n",
    "#         for token in tokens:\n",
    "#             start, end = self.get_text_start_end(full_text, token, search_from=current_offset)\n",
    "#             offset_mapping.append((start, end))\n",
    "#             current_offset = end\n",
    "\n",
    "#         return offset_mapping\n",
    "#     # ======================================================================================== #\n",
    "#     def create_mapper_n_clean(self,full_text,labels,offset_mapping,attribut=[\"NAME_STUDENT\"]):\n",
    "#         ent,ent_order,ent_offset_in_order = self.generate_ent(full_text,labels,offset_mapping)\n",
    "\n",
    "#         mapper = {}\n",
    "#         label_mapper = {}\n",
    "#         new_tokens = []\n",
    "#         txt_added = 0\n",
    "#         for num,k in enumerate(ent_order):\n",
    "#             v = ent[k]\n",
    "\n",
    "#             if v in attribut:      \n",
    "#                 dc_ent = self.generate_random_data_with_probabilities()\n",
    "#                 mapper[k] = dc_ent[v]\n",
    "#                 label_mapper[dc_ent[v]] = v\n",
    "\n",
    "#                 old_len = len(full_text)\n",
    "#                 if k in mapper.keys():\n",
    "#                     full_text = full_text[:ent_offset_in_order[num][0]+txt_added] +\" \" +mapper[k] + \" \"+full_text[ent_offset_in_order[num][-1]+txt_added:]\n",
    "#                     txt_added+= len(full_text)-old_len\n",
    "\n",
    "#                     new_tokens.append(mapper[k])\n",
    "#                 else:\n",
    "#                     full_text = full_text[:ent_offset_in_order[num][0]+txt_added] + \" \"+dc_ent[v] +\" \"+ full_text[ent_offset_in_order[num][-1]+txt_added:]\n",
    "\n",
    "#                     new_tokens.append(dc_ent[v])\n",
    "#                     txt_added+= len(full_text)-old_len\n",
    "#             else:\n",
    "#                 label_mapper[k] = v\n",
    "#                 new_tokens.append(k)\n",
    "\n",
    "#         full_text = self.clean_text(full_text)\n",
    "\n",
    "#         tokenized_text = self.tokenize_with_spacy(full_text)\n",
    "#         tokens = tokenized_text['tokens']\n",
    "#         tg = self.get_offset_mapping(full_text, new_tokens)\n",
    "\n",
    "\n",
    "#         woff = np.array(tokenized_text['offset_mapping'])\n",
    "#         labels = np.array([\"O\"]*len(woff),dtype='<U50')\n",
    "\n",
    "#         toff = np.array(tg)\n",
    "#         wx1, wx2 = woff.T\n",
    "#         tx1, tx2 = toff.T\n",
    "#         ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n",
    "#         ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n",
    "#         ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n",
    "#         ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n",
    "#         ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1)\n",
    "\n",
    "\n",
    "#         for i,row in enumerate(ious):\n",
    "#             inds = row.nonzero()[0]\n",
    "#             if len(inds):\n",
    "#                 labels[i] = label_mapper[new_tokens[inds[0]]]\n",
    "\n",
    "#         labels = labels.tolist()\n",
    "\n",
    "#         return full_text,labels,tokens\n",
    "\n",
    "#     # ======================================================================================== #\n",
    "#     def get_text_start_end(self,txt, s, search_from=0):\n",
    "#         txt = txt[int(search_from):]\n",
    "#         try:\n",
    "#             idx = txt.find(s)\n",
    "#             if idx >= 0:\n",
    "#                 st = idx\n",
    "#                 ed = st + len(s)\n",
    "#             else:\n",
    "#                 raise ValueError('Error')\n",
    "#         except:\n",
    "#             res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "#             if len(res):\n",
    "#                 st, ed = res[0][0], res[0][1]\n",
    "#             else:\n",
    "#                 m = SequenceMatcher(None, s, txt).get_opcodes()\n",
    "#                 for tag, i1, i2, j1, j2 in m:\n",
    "#                     if tag == 'replace':\n",
    "#                         s = s[:i1] + txt[j1:j2] + s[i2:]\n",
    "#                     if tag == \"delete\":\n",
    "#                         s = s[:i1] + s[i2:]\n",
    "\n",
    "#                 res = [(m.start(0), m.end(0)) for m in re.finditer(s, txt)]\n",
    "#                 if len(res):\n",
    "#                     st, ed = res[0][0], res[0][1]\n",
    "#                 else:\n",
    "#                     idx = txt.find(s)\n",
    "#                     if idx >= 0:\n",
    "#                         st = idx\n",
    "#                         ed = st + len(s)\n",
    "#                     else:\n",
    "#                         st, ed = 0, 0\n",
    "#         return st + search_from, ed + search_from\n",
    "    \n",
    "#     # ======================================================================================== #\n",
    "#     def find_patterns(self,text,regex):\n",
    "#         matches = [(match.group(0), match.start(), match.end()) for match in regex.finditer(text)]\n",
    "#         offsets = self.strip_offset_mapping(text,[(m[1],m[2]) for m in matches])\n",
    "#         return [m[0].strip() for m in matches],offsets\n",
    "#     # ======================================================================================== #\n",
    "#     def spacy_to_re_off(self,text,tokens,offset_mapping_init):\n",
    "        \n",
    "\n",
    "#         lab,off_matc = self.find_patterns(text,REGEX_COMPILE)\n",
    "\n",
    "\n",
    "#         if len(lab):\n",
    "\n",
    "#             spacy_to_re = []\n",
    "#             re_oof = []\n",
    "#             tokens_re = []\n",
    "\n",
    "#             # token slices of words\n",
    "#             woff = np.array(offset_mapping_init)\n",
    "#             toff = off_matc\n",
    "#             wx1, wx2 = woff.T\n",
    "#             tx1, tx2 = toff.T\n",
    "#             ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n",
    "#             ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n",
    "#             ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n",
    "#             ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n",
    "#             ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1+1e-12)\n",
    "\n",
    "#             for i,(spcy_of_set,tok,row) in enumerate(zip(offset_mapping_init,tokens,ious)):\n",
    "#                 inds = row.nonzero()[0]\n",
    "#                 if len(inds):\n",
    "#                     spacy_to_re.append(inds[0])\n",
    "#                     re_oof.append((off_matc[inds[0]].tolist()[0],off_matc[inds[0]].tolist()[1]))\n",
    "#                     tokens_re.append(lab[inds[0]] if len(lab[inds[0]])>len(tok) else tok)\n",
    "#                 else:\n",
    "#                     spacy_to_re.append(len(lab)+i)\n",
    "#                     re_oof.append(spcy_of_set)\n",
    "#                     tokens_re.append(tok)\n",
    "\n",
    "#             re_oof = [(x,i) for i, x in enumerate(re_oof) if re_oof.index(x) == i]\n",
    "#             spacy_to_re_unique = [spacy_to_re[x[1]] for x in re_oof]\n",
    "#             re_oof = [x[0] for x in re_oof]\n",
    "#         else:\n",
    "#             spacy_to_re = np.arange(len(offset_mapping_init)).tolist()\n",
    "#             re_oof = offset_mapping_init\n",
    "#             tokens_re = tokens\n",
    "\n",
    "#         return re_oof,spacy_to_re,tokens_re,spacy_to_re_unique\n",
    "#     # ======================================================================================== #\n",
    "#     def strip_offset_mapping(self, text, offset_mapping):\n",
    "#         ret = []\n",
    "#         for start, end in offset_mapping:\n",
    "#             match = list(re.finditer('\\S+', text[start:end]))\n",
    "#             if len(match) == 0:\n",
    "#                 ret.append((start, end))\n",
    "#             else:\n",
    "#                 span_start, span_end = match[0].span()\n",
    "#                 ret.append((start + span_start, start + span_end))\n",
    "#         return np.array(ret)\n",
    "#     # ======================================================================================== #\n",
    "#     def get_word_offsets(self, text):\n",
    "#         matches = re.finditer(\"\\S+\", text)\n",
    "#         spans = []\n",
    "#         words = []\n",
    "#         for match in matches:\n",
    "#             span = match.span()\n",
    "#             word = match.group()\n",
    "#             spans.append(span)\n",
    "#             words.append(word)\n",
    "#         assert tuple(words) == tuple(text.split())\n",
    "#         return np.array(spans)\n",
    "#     # ======================================================================================== #\n",
    "#     def custom_distribution(self,n):\n",
    "#         distribution = [0] * n\n",
    "#         middle_index = n // 2\n",
    "#         for i in range(middle_index):\n",
    "#             distribution[i] = (middle_index - i) / middle_index\n",
    "#             distribution[n - 1 - i] = (middle_index - i) / middle_index\n",
    "#         return distribution\n",
    "#     # ======================================================================================== #\n",
    "#     def add_text(self,full_text,tokens,labels,offset_mapping,\n",
    "#                  new_text,new_tokens,new_labels,new_offset_mapping):\n",
    "#         try:\n",
    "#             s = full_text.split('|')\n",
    "#             prob_dist = self.custom_distribution(len(s))\n",
    "#             id_ = random.choices(np.arange(len(s)),k=1,weights = prob_dist)[0]\n",
    "\n",
    "#             idx = [len(s[i]) for i in range(id_+1)]\n",
    "#             idx = sum(idx)\n",
    "#             full_text = full_text[:idx+id_+1] +\" \"+ new_text + full_text[idx+id_+1:]\n",
    "\n",
    "\n",
    "#             t_idx = [i for i,x in enumerate(offset_mapping) if x[1]==idx+id_+1][-1]+1\n",
    "\n",
    "\n",
    "#             tokens = tokens[:t_idx]+new_tokens+tokens[t_idx:]\n",
    "#             labels = labels[:t_idx]+new_labels+labels[t_idx:]\n",
    "\n",
    "\n",
    "#             v = offset_mapping[:t_idx][-1][1]\n",
    "#             new_offset_mappings = [(x[0]+v+1,x[1]+v+1) for x in new_offset_mapping]\n",
    "#             v1 = new_offset_mappings[-1][1]\n",
    "#             vx = v1-v\n",
    "            \n",
    "#             old_offset_mapping =  [(x[0]+vx,x[1]+vx) for x in offset_mapping[t_idx:]]\n",
    "#             offset_mapping = offset_mapping[:t_idx]+new_offset_mappings+old_offset_mapping\n",
    "\n",
    "#         except:\n",
    "#             print(\"Text not added\")\n",
    "#         return full_text,tokens,labels,offset_mapping\n",
    "\n",
    "# ## =============================================================================== ##\n",
    "# ##                                                                                 ##\n",
    "# ## =============================================================================== ##\n",
    "\n",
    "# class CustomCollator(object):\n",
    "#     def __init__(self, tokenizer, model):\n",
    "#         self.pad_token_id = tokenizer.pad_token_id\n",
    "#         if hasattr(model.config, 'attention_window'):\n",
    "#             # For longformer\n",
    "#             # https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/longformer/modeling_longformer.py#L1548\n",
    "#             self.attention_window = (model.config.attention_window\n",
    "#                                      if isinstance(\n",
    "#                                          model.config.attention_window, int)\n",
    "#                                      else max(model.config.attention_window))\n",
    "#         else:\n",
    "#             self.attention_window = None\n",
    "\n",
    "#     def __call__(self, samples):\n",
    "#         batch_size = len(samples)\n",
    "#         assert batch_size == 1, f'Only batch_size=1 supported, got batch_size={batch_size}.'\n",
    "\n",
    "#         sample = samples[0]\n",
    "\n",
    "#         max_seq_length = len(sample['input_ids'])\n",
    "#         if self.attention_window is not None:\n",
    "#             attention_window = self.attention_window\n",
    "#             padded_length = (attention_window -\n",
    "#                              max_seq_length % attention_window\n",
    "#                              ) % attention_window + max_seq_length\n",
    "#         else:\n",
    "#             padded_length = max_seq_length\n",
    "\n",
    "#         input_shape = (1, padded_length)\n",
    "#         input_ids = torch.full(input_shape,\n",
    "#                                self.pad_token_id,\n",
    "#                                dtype=torch.long)\n",
    "#         attention_mask = torch.zeros(input_shape, dtype=torch.long)\n",
    "\n",
    "#         seq_length = len(sample['input_ids'])\n",
    "#         input_ids[0, :seq_length] = sample['input_ids']\n",
    "#         attention_mask[0, :seq_length] = sample['attention_mask']\n",
    "\n",
    "#         text_id = sample['text_id']\n",
    "#         tokens = sample['tokens']\n",
    "#         tokens_clean = sample['tokens_clean']\n",
    "#         re_tokens = sample['re_tokens']\n",
    "#         spacy_to_re = sample['spacy_to_re']\n",
    "#         spacy_to_re_unique = sample['spacy_to_re_unique']\n",
    "#         word_boxes = sample['word_boxes']\n",
    "#         gt_spans = sample['gt_spans']\n",
    "\n",
    "#         return dict(text_id=text_id,\n",
    "#                     tokens_clean=tokens_clean,\n",
    "#                     tokens = tokens,\n",
    "#                     re_tokens = re_tokens,\n",
    "#                     spacy_to_re = spacy_to_re,\n",
    "#                     spacy_to_re_unique = spacy_to_re_unique,\n",
    "#                     input_ids=input_ids,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     word_boxes=word_boxes,\n",
    "#                     gt_spans=gt_spans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
